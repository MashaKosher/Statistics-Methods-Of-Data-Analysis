## Лабораторная работа 3 — Простая линейная регрессия (Jupyter Notebook)

Этот файл описывает, что именно делается в ноутбуке `lab3.ipynb`, в каком порядке, зачем это нужно и как интерпретировать результаты.

## 1. Исходные данные
- Формируется таблица наблюдений с колонками: `t` (номер наблюдения), `tx` (объясняющая переменная x), `ty` (отклик y).
- Эти данные будут использоваться для оценки простой линейной регрессии: \( y = a_0 + a_1 x + \varepsilon \).

## 2. Точечный график зависимости ty от tx
- Строится scatter plot `ty` от `tx` для визуальной проверки линейности связи и наличия выбросов.
- Масштабы осей устанавливаются: X от 12 до 20, Y от 200 до 300.
- Зачем: предварительная визуальная диагностика, оценка знака наклона.

## 3. Предварительная оценка коэффициентов модели
- Оцениваются \(\hat a_0\) и \(\hat a_1\) (например, через `numpy.polyfit` или формулы МНК).
- В нашем наборе: \(\hat a_1\) положительный (линейный рост), \(\hat a_0\) положительный.

## 4. Расширенная таблица признаков
- Добавляются столбцы:
  - `tx^2`
  - `(tx-x̄)^2`, `(ty-ȳ)^2`
  - `(tx-x̄)(ty-ȳ)`
- Зачем: эти величины используются для суммарных характеристик \(S_{xx}\), \(S_{xy}\) и дальнейших расчётов.

## 5. Оценка параметров по классическим формулам
- Вычисляются выборочные средние \(\bar x\), \(\bar y\).
- Суммы:
  - \(S_{xx} = \sum (x_t - \bar x)^2\)
  - \(S_{xy} = \sum (x_t - \bar x)(y_t - \bar y)\)
- Оценки:
  - \(\hat a_1 = S_{xy} / S_{xx}\)
  - \(\hat a_0 = \bar y - \hat a_1 \bar x\)

## 6. Прогнозы, ошибки и вклад в оценку дисперсии
- Строится столбец прогнозов: \(\hat y_t = \hat a_0 + \hat a_1 x_t\).
- Остатки: \(e_t = y_t - \hat y_t\), квадраты остатков: \(e_t^2\).
- Вклад в оценку дисперсии: \(s_t^2 = e_t^2 / (T-2)\). Добавляются строки «сумма» и «среднее».

## 7. Несмещённая оценка дисперсии случайных ошибок
- \(\sigma^2\,\hat{} = \text{SSE}/(T-2)\), где \(\text{SSE} = \sum e_t^2\).
- Зачем: базовая дисперсионная характеристика шума модели.

## 8. Дисперсии оценок коэффициентов
- \(\operatorname{Var}(\hat a_1) = s^2 / S_{xx}\).
- \(\operatorname{Var}(\hat a_0) = s^2\,\big(1/T + \bar x^2/S_{xx}\big)\).
- Также выводятся стандартные ошибки как корни из дисперсий.

## 9. Квантили распределения Стьюдента и t-статистики
- Для уровней доверия \(\gamma\in\{0.90, 0.95, 0.99\}\) при \(\text{df}=T-2\) берутся двусторонние множители \(t_{(1+\gamma)/2,\,T-2}\).
- Вычисляются наблюдаемые t-статистики для \(\hat a_1\) и \(\hat a_0\): отношение оценки к её стандартной ошибке.
- Сравнение с критическими значениями позволяет сделать вывод о статистической значимости коэффициентов.

## 10. Доверительные интервалы для коэффициентов
- Для \(\hat a_1\) и \(\hat a_0\) строятся двусторонние интервалы вида:
  - \([\hat a - t\,\cdot\operatorname{se}(\hat a),\; \hat a + t\,\cdot\operatorname{se}(\hat a)]\).
- Интервалы строятся для уровней 0.90, 0.95, 0.99.

## 11. Коэффициент детерминации R² и адекватность
- \(R^2 = 1 - \text{SSE}/\text{SST}\), где \(\text{SST} = \sum (y_t-\bar y)^2\).
- Даётся краткое заключение (высокая/умеренная/низкая объясняющая способность) на основе величины \(R^2\).

## 12. F-квантили и F-тест значимости R²
- Рассчитываются верхние квантили Фишера \(F_{\gamma; (1,\,T-2)}\) или \(F_{\gamma; (2,\,T-2)}\) в зависимости от постановки.
- Наблюдаемое значение: \(F_R = (R^2/(1-R^2))\cdot(T-2)\) при df1=1, df2=T−2.
- Сравнение с критическими значениями даёт вывод о значимости \(R^2\).

## 13. Таблица прогнозов для x = 12..20
- Для целых значений `tx` от 12 до 20 вычисляются прогнозы \(\hat y\).
- Результат выводится в отдельной таблице.

## 14. Доверительные интервалы прогнозов (предиктивные интервалы)
- Для каждого \(x_0\) строится интервал:
  - \(\hat y(x_0) \pm t_{(1+\gamma)/2,\,T-2}\cdot \sqrt{s^2\,\big(1 + 1/T + (x_0-\bar x)^2/S_{xx}\big)}\).
- Интервалы показаны для \(\gamma = 0.90, 0.95, 0.99\).

## 15. Графики прогноза и границ интервалов
- Строится кривая \(\hat y(x)\) и для каждого уровня \(\gamma\) — верхняя/нижняя границы предиктивного интервала.
- Масштабы осей: X [12, 20], Y [190, 310].

## 16. Анализ остатков
- Строится точечный график остатков \(e_t\) по времени t с линией уровня 0.
- Дополнительно: QQ-plot и (если доступен `scipy`) тест Шапиро–Уилка для проверки нормальности остатков.
- Зачем: проверка предпосылок МНК (нормальность/гомоскедастичность/отсутствие явной автокорреляции).

## 17. Вспомогательные утилиты (`helpers`)
- `helpers/graphics.py`: функции визуализации (scatter, линии регрессии и т.п.).
- `helpers/calc.py`: класс `DFParams` с методами:
  - `x_bar(tx)`, `y_bar(ty)` — выборочные средние \(\bar x\), \(\bar y\)
  - `sxx(tx)` — \(S_{xx} = \sum (x-\bar x)^2\)
  - `sxy(tx, ty)` — \(S_{xy} = \sum (x-\bar x)(y-\bar y)\)

## 18. Как запустить
1. Откройте `lab3/lab3.ipynb` в Jupyter/VS Code/Cursor.
2. Выполните ячейки сверху вниз (или Run All).
3. Зависимости: `pandas`, `numpy`, `matplotlib`. Необязательно, но желательно: `scipy` (для t/F квантилей и теста Шапиро–Уилка). При отсутствии `scipy` используются табличные/нормальные аппроксимации и выводятся пояснения.

## 19. Интерпретация результатов (кратко)
- Положительный \(\hat a_1\) подтверждает рост `ty` с увеличением `tx`.
- Высокий \(R^2\) указывает на хорошую объясняющую способность линейной модели.
- Значимость \(\hat a_0\), \(\hat a_1\) — по t-тестам; значимость \(R^2\) — по F-тесту.
- Предиктивные интервалы показывают неопределённость будущих наблюдений вокруг прогноза \(\hat y(x)\).


