{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Анализ датасета TikTok\n",
        "\n",
        "Первая часть: Загрузка данных, преобразование типов и обработка пропусков\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Импорт необходимых библиотек\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "import openpyxl\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Настройка визуализации\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_rows', 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ЗАГРУЗКА И ОЗНАКОМЛЕНИЕ С ДАННЫМИ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. ЗАГРУЗКА ДАННЫХ\n",
        "print(\"=\" * 80)\n",
        "print(\"1. ЗАГРУЗКА И ОЗНАКОМЛЕНИЕ С ДАННЫМИ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df = pd.read_excel('./tiktok_dataset.xlsm')\n",
        "\n",
        "print(f\"Размер датасета: {df.shape}\")\n",
        "print(f\"Количество строк: {df.shape[0]}\")\n",
        "print(f\"Количество столбцов: {df.shape[1]}\")\n",
        "\n",
        "# Просмотр структуры данных\n",
        "print(\"\\nПервые 5 строк датасета:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nИнформация о типах данных:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nБазовые статистики числовых переменных:\")\n",
        "print(df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.5 ПРЕОБРАЗОВАНИЕ ТИПОВ ДАННЫХ И ОБРАБОТКА ПРОПУСКОВ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.5 ПРЕОБРАЗОВАНИЕ ТИПОВ ДАННЫХ И ОБРАБОТКА ПРОПУСКОВ\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"1.5 ПРЕОБРАЗОВАНИЕ ТИПОВ ДАННЫХ И ОБРАБОТКА ПРОПУСКОВ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Проверяем типы данных\n",
        "print(\"\\nТипы данных перед обработкой:\")\n",
        "for col in df.columns:\n",
        "    print(f\"  {col}: {df[col].dtype}\")\n",
        "\n",
        "# Функция для преобразования типов данных\n",
        "def convert_column_types(df):\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    # Автоматически определяем числовые колонки\n",
        "    # Сначала пробуем преобразовать все колонки, которые выглядят как числовые\n",
        "    for col in df_clean.columns:\n",
        "        if df_clean[col].dtype == 'object':\n",
        "            # Заменяем строковые представления пропусков\n",
        "            df_clean[col] = df_clean[col].replace(['', ' ', 'NA', 'N/A', 'null', 'NULL'], np.nan)\n",
        "            # Пробуем преобразовать в числовой тип\n",
        "            converted = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "            # Если большинство значений успешно преобразовалось, заменяем колонку\n",
        "            if not converted.isna().all():\n",
        "                non_null_before = df_clean[col].notna().sum()\n",
        "                non_null_after = converted.notna().sum()\n",
        "                # Если после преобразования осталось достаточно значений, заменяем\n",
        "                if non_null_after > 0 and (non_null_after / max(non_null_before, 1)) > 0.5:\n",
        "                    df_clean[col] = converted\n",
        "                    print(f\"  Преобразуем {col} из object в числовой тип...\")\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "# Преобразуем типы данных\n",
        "df = convert_column_types(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Проверяем пропуски после преобразования\n",
        "print(\"\\nПроверка пропусков после преобразования типов:\")\n",
        "missing_after = df.isnull().sum()\n",
        "missing_cols = missing_after[missing_after > 0]\n",
        "if len(missing_cols) > 0:\n",
        "    print(\"Колонки с пропусками:\")\n",
        "    for col, count in missing_cols.items():\n",
        "        percentage = (count / len(df)) * 100\n",
        "        print(f\"  {col}: {count} пропусков ({percentage:.2f}%)\")\n",
        "        \n",
        "    # Заполняем пропуски\n",
        "    print(\"\\nЗаполнение пропусков...\")\n",
        "    \n",
        "    # Для числовых колонок заполняем медианой\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        if df[col].isnull().sum() > 0:\n",
        "            median_val = df[col].median()\n",
        "            df[col].fillna(median_val, inplace=True)\n",
        "            print(f\"  {col}: заполнено {df[col].isnull().sum()} пропусков медианой ({median_val:.2f})\")\n",
        "    \n",
        "    # Для категориальных колонок заполняем модой\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_cols:\n",
        "        if df[col].isnull().sum() > 0:\n",
        "            mode_val = df[col].mode()[0] if not df[col].mode().empty else 'Unknown'\n",
        "            df[col].fillna(mode_val, inplace=True)\n",
        "            print(f\"  {col}: заполнено {df[col].isnull().sum()} пропусков модой ('{mode_val}')\")\n",
        "else:\n",
        "    print(\"Пропусков нет!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Проверяем типы данных после обработки\n",
        "print(\"\\nТипы данных после обработки:\")\n",
        "for col in df.columns[:20]:  # Выводим первые 20 колонок\n",
        "    print(f\"  {col}: {df[col].dtype}\")\n",
        "if len(df.columns) > 20:\n",
        "    print(f\"  ... и еще {len(df.columns) - 20} колонок\")\n",
        "\n",
        "# Показываем список всех колонок\n",
        "print(f\"\\nВсего колонок в датасете: {len(df.columns)}\")\n",
        "print(\"\\nСписок всех колонок:\")\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    print(f\"  {i}. {col} ({df[col].dtype})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ОЧИСТКА И ПРЕДОБРАБОТКА ДАННЫХ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. ОЧИСТКА ДАННЫХ\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"2. ОЧИСТКА И ПРЕДОБРАБОТКА ДАННЫХ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 2.1 Проверка пропущенных значений\n",
        "print(\"\\n2.1 Проверка пропущенных значений:\")\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Количество_пропусков': missing_values,\n",
        "    'Процент_пропусков': missing_percentage\n",
        "})\n",
        "\n",
        "missing_data = missing_df[missing_df['Количество_пропусков'] > 0]\n",
        "if len(missing_data) > 0:\n",
        "    print(missing_data)\n",
        "else:\n",
        "    print(\"Пропущенных значений нет!\")\n",
        "\n",
        "# Визуализация пропущенных значений (если есть)\n",
        "if missing_values.sum() > 0:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
        "    plt.title('Матрица пропущенных значений', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('missing_values_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.2 Проверка дубликатов\n",
        "print(\"\\n2.2 Проверка дубликатов:\")\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Количество полных дубликатов: {duplicates}\")\n",
        "\n",
        "if duplicates > 0:\n",
        "    print(f\"Удаляем {duplicates} дубликатов...\")\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"Размер датасета после удаления дубликатов: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.3 Анализ баланса классов (если есть целевая переменная)\n",
        "# Ищем возможные целевые переменные\n",
        "target_candidates = [col for col in df.columns if any(keyword in col.lower() \n",
        "                    for keyword in ['target', 'label', 'class', 'outcome', 'result', 'status'])]\n",
        "\n",
        "print(\"\\n2.3 Поиск целевой переменной:\")\n",
        "if target_candidates:\n",
        "    print(f\"Найдены возможные целевые переменные: {target_candidates}\")\n",
        "    # Берем первую найденную\n",
        "    target_var = target_candidates[0]\n",
        "    print(f\"\\nАнализируем переменную: {target_var}\")\n",
        "    \n",
        "    target_counts = df[target_var].value_counts()\n",
        "    target_percentage = df[target_var].value_counts(normalize=True) * 100\n",
        "    \n",
        "    print(f\"Распределение {target_var}:\")\n",
        "    for val, count in target_counts.items():\n",
        "        print(f\"  {val}: {count} ({target_percentage[val]:.2f}%)\")\n",
        "    \n",
        "    # Визуализация баланса классов\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    colors = sns.color_palette(\"husl\", len(target_counts))\n",
        "    bars = plt.bar(range(len(target_counts)), target_counts.values, color=colors, alpha=0.8)\n",
        "    \n",
        "    plt.title(f'Распределение целевой переменной {target_var}', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel(target_var, fontweight='bold')\n",
        "    plt.ylabel('Количество наблюдений', fontweight='bold')\n",
        "    plt.xticks(range(len(target_counts)), target_counts.index, rotation=45, ha='right')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Добавление значений на столбцы\n",
        "    for bar, count in zip(bars, target_counts.values):\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                 f'{count}\\n({count/len(df)*100:.1f}%)',\n",
        "                 ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('class_balance.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Целевая переменная не найдена автоматически.\")\n",
        "    print(\"Если есть целевая переменная, укажите её вручную для дальнейшего анализа.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ПРОВЕРКА И ОЧИСТКА ДАННЫХ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Проверяем числовые колонки на NaN и inf\n",
        "print(\"\\nПроверка числовых колонок на проблемы:\")\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "problems_found = False\n",
        "\n",
        "for col in numeric_cols:\n",
        "    nan_count = df[col].isna().sum()\n",
        "    inf_count = np.isinf(df[col]).sum()\n",
        "    if nan_count > 0 or inf_count > 0:\n",
        "        problems_found = True\n",
        "        print(f\"  {col}: {nan_count} NaN, {inf_count} inf/ -inf\")\n",
        "        \n",
        "        # Заменяем inf на NaN, затем заполняем\n",
        "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
        "        if df[col].isna().sum() > 0:\n",
        "            median_val = df[col].median()\n",
        "            if pd.notna(median_val):\n",
        "                df[col].fillna(median_val, inplace=True)\n",
        "                print(f\"    Заполнено медианой: {median_val:.2f}\")\n",
        "            else:\n",
        "                # Если медиана тоже NaN, заполняем 0\n",
        "                df[col].fillna(0, inplace=True)\n",
        "                print(f\"    Заполнено нулем (медиана была NaN)\")\n",
        "\n",
        "if not problems_found:\n",
        "    print(\"Проблем с числовыми колонками не обнаружено!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ПРЕДВАРИТЕЛЬНЫЙ АНАЛИЗ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. ПРЕДВАРИТЕЛЬНЫЙ АНАЛИЗ\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"3. ПРЕДВАРИТЕЛЬНЫЙ АНАЛИЗ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 3.1 Анализ числовых переменных\n",
        "print(\"\\n3.1 Анализ числовых переменных:\")\n",
        "\n",
        "# Автоматически определяем типы переменных\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"Найдено числовых переменных: {len(numeric_cols)}\")\n",
        "print(f\"Найдено категориальных переменных: {len(categorical_cols)}\")\n",
        "\n",
        "# Для порядковых переменных - ищем колонки с небольшим количеством уникальных значений\n",
        "ordinal_vars = []\n",
        "continuous_vars = []\n",
        "\n",
        "for col in numeric_cols:\n",
        "    unique_count = df[col].nunique()\n",
        "    if unique_count <= 10 and df[col].dtype in [np.int64, np.int32]:\n",
        "        ordinal_vars.append(col)\n",
        "    else:\n",
        "        continuous_vars.append(col)\n",
        "\n",
        "print(f\"\\nНепрерывных переменных: {len(continuous_vars)}\")\n",
        "print(f\"Порядковых переменных: {len(ordinal_vars)}\")\n",
        "\n",
        "# ФИКС: Проверяем и преобразуем числовые колонки\n",
        "print(\"\\nПроверка типов данных для числовых переменных...\")\n",
        "for var in continuous_vars + ordinal_vars:\n",
        "    if var in df.columns:\n",
        "        if df[var].dtype == 'object':\n",
        "            print(f\"  Преобразуем {var} из {df[var].dtype} в числовой тип...\")\n",
        "            # Пробуем преобразовать в числовой тип, ошибки превращаем в NaN\n",
        "            df[var] = pd.to_numeric(df[var], errors='coerce')\n",
        "            \n",
        "            # Заполняем пропущенные значения медианой\n",
        "            if df[var].isnull().sum() > 0:\n",
        "                median_val = df[var].median()\n",
        "                if pd.notna(median_val):\n",
        "                    df[var].fillna(median_val, inplace=True)\n",
        "                    print(f\"    Заполнено {df[var].isnull().sum()} пропусков медианой: {median_val}\")\n",
        "                else:\n",
        "                    df[var].fillna(0, inplace=True)\n",
        "                    print(f\"    Заполнено нулем (медиана была NaN)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Визуализация распределения ключевых непрерывных переменных\n",
        "if len(continuous_vars) > 0:\n",
        "    # Берем первые 9 переменных для визуализации\n",
        "    key_continuous = continuous_vars[:9]\n",
        "    \n",
        "    # ФИКС: Фильтруем только существующие переменные\n",
        "    key_continuous = [var for var in key_continuous if var in df.columns]\n",
        "    \n",
        "    if len(key_continuous) > 0:\n",
        "        n_plots = len(key_continuous)\n",
        "        n_rows = (n_plots + 2) // 3  # Округляем вверх\n",
        "        fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5*n_rows))\n",
        "        \n",
        "        # Если только одна строка, делаем axes двумерным\n",
        "        if n_rows == 1:\n",
        "            axes = axes.reshape(1, -1)\n",
        "        \n",
        "        for idx, var in enumerate(key_continuous):\n",
        "            row = idx // 3\n",
        "            col = idx % 3\n",
        "            \n",
        "            # Проверяем, что переменная числовая\n",
        "            if pd.api.types.is_numeric_dtype(df[var]):\n",
        "                # Гистограмма с KDE\n",
        "                ax = axes[row, col]\n",
        "                \n",
        "                # ФИКС: Убедимся, что данные числовые\n",
        "                data_to_plot = pd.to_numeric(df[var], errors='coerce').dropna()\n",
        "                \n",
        "                if len(data_to_plot) > 0:\n",
        "                    sns.histplot(data_to_plot, kde=True, ax=ax, bins=30, color='skyblue', edgecolor='black')\n",
        "                    ax.axvline(data_to_plot.mean(), color='red', linestyle='--', linewidth=2, label='Среднее')\n",
        "                    ax.axvline(data_to_plot.median(), color='green', linestyle='--', linewidth=2, label='Медиана')\n",
        "                    \n",
        "                    ax.set_title(f'Распределение {var}', fontweight='bold')\n",
        "                    ax.set_xlabel('')\n",
        "                    ax.legend(fontsize=8)\n",
        "                    \n",
        "                    # Добавление статистик\n",
        "                    stats_text = f'Mean: {data_to_plot.mean():.1f}\\nMedian: {data_to_plot.median():.1f}\\nStd: {data_to_plot.std():.1f}'\n",
        "                    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, \n",
        "                            fontsize=8, verticalalignment='top',\n",
        "                            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "                else:\n",
        "                    ax.text(0.5, 0.5, f'Нет данных для {var}', \n",
        "                            transform=ax.transAxes, ha='center', va='center')\n",
        "                    ax.set_title(f'Распределение {var}', fontweight='bold')\n",
        "            else:\n",
        "                # Если переменная не числовая\n",
        "                ax = axes[row, col]\n",
        "                ax.text(0.5, 0.5, f'{var} не числовая\\nТип: {df[var].dtype}', \n",
        "                        transform=ax.transAxes, ha='center', va='center')\n",
        "                ax.set_title(f'Распределение {var}', fontweight='bold')\n",
        "        \n",
        "        # Скрываем пустые оси, если переменных меньше 9\n",
        "        for idx in range(len(key_continuous), n_rows * 3):\n",
        "            row = idx // 3\n",
        "            col = idx % 3\n",
        "            if row < n_rows:\n",
        "                axes[row, col].set_visible(False)\n",
        "        \n",
        "        plt.suptitle('Распределение ключевых непрерывных переменных', fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('continuous_vars_distribution.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Нет непрерывных переменных для визуализации\")\n",
        "else:\n",
        "    print(\"Непрерывные переменные не найдены\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.2 Анализ порядковых переменных\n",
        "print(\"\\n3.2 Анализ порядковых (ординальных) переменных:\")\n",
        "\n",
        "if len(ordinal_vars) > 0:\n",
        "    # Берем первые 9 порядковых переменных\n",
        "    vars_to_plot = ordinal_vars[:9]\n",
        "    \n",
        "    n_plots = len(vars_to_plot)\n",
        "    n_rows = (n_plots + 2) // 3\n",
        "    fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5*n_rows))\n",
        "    \n",
        "    # Если только одна строка, делаем axes двумерным\n",
        "    if n_rows == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    \n",
        "    for idx, var in enumerate(vars_to_plot):\n",
        "        if var not in df.columns:\n",
        "            continue\n",
        "            \n",
        "        row = idx // 3\n",
        "        col = idx % 3\n",
        "        ax = axes[row, col]\n",
        "        \n",
        "        value_counts = df[var].value_counts().sort_index()\n",
        "        \n",
        "        bars = ax.bar(range(len(value_counts)), value_counts.values, \n",
        "                      color=plt.cm.Set3(np.arange(len(value_counts))/len(value_counts)))\n",
        "        ax.set_title(f'{var} Distribution', fontweight='bold')\n",
        "        ax.set_xlabel('Уровень/Оценка')\n",
        "        ax.set_ylabel('Количество')\n",
        "        ax.set_xticks(range(len(value_counts)))\n",
        "        ax.set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
        "        \n",
        "        # Добавление процентов\n",
        "        total = len(df)\n",
        "        for i, (x_pos, count) in enumerate(zip(range(len(value_counts)), value_counts.values)):\n",
        "            percentage = (count / total) * 100\n",
        "            ax.text(x_pos, count + 0.02 * max(value_counts.values), \n",
        "                    f'{percentage:.1f}%', ha='center', va='bottom', fontsize=8)\n",
        "    \n",
        "    # Скрываем пустые оси\n",
        "    for idx in range(len(vars_to_plot), n_rows * 3):\n",
        "        row = idx // 3\n",
        "        col = idx % 3\n",
        "        if row < n_rows:\n",
        "            axes[row, col].set_visible(False)\n",
        "    \n",
        "    plt.suptitle('Распределение порядковых переменных', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('ordinal_vars_distribution.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Порядковые переменные не найдены\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.3 Сравнение по группам (если есть целевая переменная)\n",
        "print(\"\\n3.3 Сравнение ключевых показателей по группам:\")\n",
        "\n",
        "# Ищем целевую переменную\n",
        "target_var = None\n",
        "if target_candidates:\n",
        "    target_var = target_candidates[0]\n",
        "    print(f\"Используем целевую переменную: {target_var}\")\n",
        "    \n",
        "    # Получаем уникальные значения целевой переменной\n",
        "    target_values = df[target_var].unique()\n",
        "    print(f\"Группы в целевой переменной: {target_values}\")\n",
        "    \n",
        "    if len(target_values) >= 2:\n",
        "        # Берем первые 9 числовых переменных для сравнения\n",
        "        comparison_vars = continuous_vars[:9] if len(continuous_vars) >= 9 else continuous_vars\n",
        "        \n",
        "        # Фильтруем только существующие и числовые переменные\n",
        "        valid_comparison_vars = []\n",
        "        for var in comparison_vars:\n",
        "            if var in df.columns and pd.api.types.is_numeric_dtype(df[var]):\n",
        "                valid_comparison_vars.append(var)\n",
        "        \n",
        "        print(f\"Анализируем {len(valid_comparison_vars)} переменных\")\n",
        "        \n",
        "        if len(valid_comparison_vars) > 0:\n",
        "            n_plots = min(len(valid_comparison_vars), 9)\n",
        "            n_rows = (n_plots + 2) // 3\n",
        "            fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5*n_rows))\n",
        "            \n",
        "            # Если только одна строка, делаем axes двумерным\n",
        "            if n_rows == 1:\n",
        "                axes = axes.reshape(1, -1)\n",
        "            \n",
        "            for idx, var in enumerate(valid_comparison_vars):\n",
        "                if idx >= 9:  # Максимум 9 графиков\n",
        "                    break\n",
        "                    \n",
        "                row = idx // 3\n",
        "                col = idx % 3\n",
        "                ax = axes[row, col]\n",
        "                \n",
        "                try:\n",
        "                    # Создаем данные для boxplot\n",
        "                    plot_data = []\n",
        "                    groups = []\n",
        "                    \n",
        "                    # Берем первые два значения целевой переменной для сравнения\n",
        "                    for target_val in target_values[:2]:\n",
        "                        # Фильтруем данные для каждой группы\n",
        "                        mask = (df[target_var] == target_val)\n",
        "                        subset = df.loc[mask, var]\n",
        "                        \n",
        "                        # Удаляем NaN и бесконечные значения\n",
        "                        subset_clean = subset.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "                        \n",
        "                        if len(subset_clean) > 0:\n",
        "                            plot_data.append(subset_clean)\n",
        "                            groups.append(str(target_val))\n",
        "                    \n",
        "                    if len(plot_data) == 2 and len(plot_data[0]) > 0 and len(plot_data[1]) > 0:\n",
        "                        # Boxplot\n",
        "                        bp = ax.boxplot(plot_data, labels=groups, patch_artist=True, widths=0.6)\n",
        "                        \n",
        "                        # Настройка цветов\n",
        "                        colors = ['#FF6B6B', '#4ECDC4']\n",
        "                        for patch, color in zip(bp['boxes'], colors[:len(bp['boxes'])]):\n",
        "                            patch.set_facecolor(color)\n",
        "                            patch.set_alpha(0.7)\n",
        "                        \n",
        "                        # Медианы\n",
        "                        for median in bp['medians']:\n",
        "                            median.set(color='black', linewidth=2)\n",
        "                        \n",
        "                        ax.set_title(f'{var} по {target_var}', fontweight='bold')\n",
        "                        ax.set_ylabel(var)\n",
        "                        ax.grid(True, alpha=0.3, axis='y')\n",
        "                        \n",
        "                        # Добавление t-test статистики\n",
        "                        try:\n",
        "                            group1_data = df[df[target_var] == target_values[0]][var].replace([np.inf, -np.inf], np.nan).dropna()\n",
        "                            group2_data = df[df[target_var] == target_values[1]][var].replace([np.inf, -np.inf], np.nan).dropna()\n",
        "                            \n",
        "                            if len(group1_data) > 1 and len(group2_data) > 1:\n",
        "                                t_stat, p_value = stats.ttest_ind(group1_data, group2_data, equal_var=False, nan_policy='omit')\n",
        "                                stars = '***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else 'ns'\n",
        "                                ax.text(0.5, 0.95, f'p = {p_value:.4f} {stars}', \n",
        "                                        transform=ax.transAxes, ha='center', va='top',\n",
        "                                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "                                \n",
        "                                # Вывод статистики в консоль\n",
        "                                print(f\"{var}: {target_values[0]} (n={len(group1_data)}): mean={group1_data.mean():.2f}, \"\n",
        "                                      f\"{target_values[1]} (n={len(group2_data)}): mean={group2_data.mean():.2f}, p={p_value:.4f}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"  Ошибка t-теста для {var}: {str(e)}\")\n",
        "                    else:\n",
        "                        ax.text(0.5, 0.5, 'Недостаточно данных', \n",
        "                                transform=ax.transAxes, ha='center', va='center')\n",
        "                        ax.set_title(f'{var} по {target_var}', fontweight='bold')\n",
        "                        ax.set_ylabel(var)\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    ax.text(0.5, 0.5, f'Ошибка: {str(e)[:30]}...', \n",
        "                            transform=ax.transAxes, ha='center', va='center')\n",
        "                    ax.set_title(f'{var} по {target_var}', fontweight='bold')\n",
        "                    ax.set_ylabel(var)\n",
        "            \n",
        "            # Скрываем пустые оси\n",
        "            for idx in range(len(valid_comparison_vars), n_rows * 3):\n",
        "                row = idx // 3\n",
        "                col = idx % 3\n",
        "                if row < n_rows:\n",
        "                    axes[row, col].set_visible(False)\n",
        "            \n",
        "            plt.suptitle(f'Сравнение показателей по {target_var} (с t-тестами)', fontsize=16, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('target_comparison_ttest.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Нет подходящих переменных для сравнения\")\n",
        "    else:\n",
        "        print(f\"Целевая переменная имеет менее 2 групп ({len(target_values)}), сравнение невозможно\")\n",
        "else:\n",
        "    print(\"Целевая переменная не найдена, пропускаем сравнение по группам\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. КОРРЕЛЯЦИОННЫЙ АНАЛИЗ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. КОРРЕЛЯЦИОННЫЙ АНАЛИЗ\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"4. КОРРЕЛЯЦИОННЫЙ АНАЛИЗ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Создаем числовую версию целевой переменной (если найдена)\n",
        "df_numeric = df.copy()\n",
        "\n",
        "# Создаем числовую версию целевой переменной\n",
        "target_var_numeric = None\n",
        "if target_candidates:\n",
        "    target_var = target_candidates[0]\n",
        "    target_var_numeric = f\"{target_var}_numeric\"\n",
        "    \n",
        "    # Приводим к стандартному виду\n",
        "    if df_numeric[target_var].dtype == 'object':\n",
        "        df_numeric[target_var] = df_numeric[target_var].astype(str).str.strip().str.title()\n",
        "        \n",
        "        # Создаем числовую версию\n",
        "        # Определяем уникальные значения и создаем маппинг\n",
        "        unique_vals = df_numeric[target_var].unique()\n",
        "        target_map = {}\n",
        "        for i, val in enumerate(sorted(unique_vals)):\n",
        "            target_map[val] = i\n",
        "        \n",
        "        df_numeric[target_var_numeric] = df_numeric[target_var].map(target_map)\n",
        "        \n",
        "        # Проверяем результат\n",
        "        print(f\"Уникальные значения {target_var_numeric}: {df_numeric[target_var_numeric].unique()}\")\n",
        "        \n",
        "        # Если есть NaN, заполняем наиболее частым значением\n",
        "        if df_numeric[target_var_numeric].isnull().sum() > 0:\n",
        "            print(f\"Предупреждение: {df_numeric[target_var_numeric].isnull().sum()} строк с некорректными значениями {target_var}\")\n",
        "            mode_val = df_numeric[target_var_numeric].mode()[0] if not df_numeric[target_var_numeric].mode().empty else 0\n",
        "            df_numeric[target_var_numeric].fillna(mode_val, inplace=True)\n",
        "            print(f\"  Заполнено значением: {mode_val}\")\n",
        "    else:\n",
        "        # Если уже числовая, просто копируем\n",
        "        df_numeric[target_var_numeric] = df_numeric[target_var]\n",
        "        print(f\"Целевая переменная {target_var} уже числовая\")\n",
        "else:\n",
        "    print(\"Целевая переменная не найдена, корреляционный анализ будет без целевой переменной\")\n",
        "\n",
        "# Выбираем числовые колонки для корреляционного анализа\n",
        "numeric_for_corr = []\n",
        "for var in continuous_vars + ordinal_vars:\n",
        "    if var in df_numeric.columns and pd.api.types.is_numeric_dtype(df_numeric[var]):\n",
        "        numeric_for_corr.append(var)\n",
        "\n",
        "# Добавляем целевую переменную, если она есть\n",
        "if target_var_numeric and target_var_numeric in df_numeric.columns:\n",
        "    numeric_for_corr.append(target_var_numeric)\n",
        "\n",
        "print(f\"\\nИспользуем {len(numeric_for_corr)} числовых переменных для корреляционного анализа\")\n",
        "\n",
        "# Проверяем, что достаточно данных\n",
        "if len(numeric_for_corr) < 2:\n",
        "    print(\"ОШИБКА: Недостаточно числовых переменных для корреляционного анализа!\")\n",
        "else:\n",
        "    # Создаем корреляционную матрицу\n",
        "    corr_matrix = df_numeric[numeric_for_corr].corr()\n",
        "    \n",
        "    print(f\"\\nРазмер корреляционной матрицы: {corr_matrix.shape}\")\n",
        "    \n",
        "    # 4.1 Тепловая карта всех корреляций\n",
        "    plt.figure(figsize=(16, 14))\n",
        "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "    \n",
        "    sns.heatmap(corr_matrix, mask=mask, cmap=cmap, center=0,\n",
        "                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
        "                annot=False)\n",
        "    \n",
        "    plt.title('Тепловая карта корреляций между числовыми переменными', \n",
        "              fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('full_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.2 Корреляции с целевой переменной (если есть)\n",
        "if 'corr_matrix' in locals() and target_var_numeric and target_var_numeric in corr_matrix.columns and 'target_var' in locals():\n",
        "    print(f\"\\n4.2 Переменные с наибольшей корреляцией с {target_var}:\")\n",
        "    target_corr = corr_matrix[target_var_numeric].sort_values(ascending=False)\n",
        "    \n",
        "    # Топ положительных и отрицательных корреляций\n",
        "    print(\"\\nТоп-10 положительных корреляций:\")\n",
        "    top_pos = target_corr.head(11)  # 11 потому что первое - сама целевая переменная\n",
        "    print(top_pos.to_string())\n",
        "    \n",
        "    print(\"\\nТоп-10 отрицательных корреляций:\")\n",
        "    top_neg = target_corr.tail(10).sort_values(ascending=True)\n",
        "    print(top_neg.to_string())\n",
        "    \n",
        "    # Визуализация топ-15 корреляций с целевой переменной\n",
        "    # Исключаем саму целевую переменную\n",
        "    top_corr_values = target_corr.drop(target_var_numeric, errors='ignore')\n",
        "    \n",
        "    if len(top_corr_values) > 0:\n",
        "        # Берем топ 8 положительных и топ 7 отрицательных\n",
        "        n_pos = min(8, len(top_corr_values[top_corr_values > 0]))\n",
        "        n_neg = min(7, len(top_corr_values[top_corr_values < 0]))\n",
        "        \n",
        "        top_pos = top_corr_values[top_corr_values > 0].head(n_pos)\n",
        "        top_neg = top_corr_values[top_corr_values < 0].tail(n_neg)\n",
        "        \n",
        "        top_corr = pd.concat([top_pos, top_neg]).sort_values()\n",
        "        \n",
        "        plt.figure(figsize=(12, 8))\n",
        "        \n",
        "        colors = ['red' if x < 0 else 'green' for x in top_corr.values]\n",
        "        bars = plt.barh(range(len(top_corr)), top_corr.values, color=colors, alpha=0.7)\n",
        "        \n",
        "        plt.yticks(range(len(top_corr)), top_corr.index)\n",
        "        plt.xlabel('Коэффициент корреляции Пирсона', fontweight='bold')\n",
        "        plt.title(f'Топ корреляций с {target_var}', fontsize=16, fontweight='bold')\n",
        "        plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "        plt.grid(axis='x', alpha=0.3)\n",
        "        \n",
        "        # Добавление значений\n",
        "        for i, (bar, val) in enumerate(zip(bars, top_corr.values)):\n",
        "            plt.text(val + (0.01 if val >= 0 else -0.03), i, \n",
        "                     f'{val:.3f}', \n",
        "                     va='center', \n",
        "                     color='black' if abs(val) < 0.15 else 'white',\n",
        "                     fontweight='bold')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('top_correlations_with_target.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"\\nНет корреляций для визуализации\")\n",
        "else:\n",
        "    print(f\"\\n{target_var_numeric if target_var_numeric else 'Целевая переменная'} не найдена в корреляционной матрице\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. АНАЛИЗ КАТЕГОРИАЛЬНЫХ ПЕРЕМЕННЫХ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. АНАЛИЗ КАТЕГОРИАЛЬНЫХ (БИНАРНЫХ) ПЕРЕМЕННЫХ\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"5. АНАЛИЗ КАТЕГОРИАЛЬНЫХ ПЕРЕМЕННЫХ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Определяем бинарные переменные (категориальные с небольшим количеством уникальных значений)\n",
        "binary_vars = []\n",
        "for col in categorical_cols:\n",
        "    unique_count = df[col].nunique()\n",
        "    if 2 <= unique_count <= 10:  # Бинарные или категориальные с небольшим количеством значений\n",
        "        binary_vars.append(col)\n",
        "\n",
        "print(f\"\\nНайдено {len(binary_vars)} категориальных переменных для анализа\")\n",
        "\n",
        "# 5.1 Анализ влияния бинарных переменных на целевую переменную\n",
        "if target_candidates:\n",
        "    target_var = target_candidates[0]\n",
        "    print(f\"\\n5.1 Влияние категориальных переменных на {target_var}:\")\n",
        "    \n",
        "    # Функция для расчета отношения шансов (Odds Ratio)\n",
        "    def calculate_odds_ratio(df, variable, target):\n",
        "        try:\n",
        "            # Создаем временный DataFrame\n",
        "            temp_df = df[[variable, target]].copy()\n",
        "            \n",
        "            # Убедимся, что target в правильном формате\n",
        "            if temp_df[target].dtype == 'object':\n",
        "                # Преобразуем target в бинарный\n",
        "                temp_df[target] = temp_df[target].astype(str).str.strip().str.title()\n",
        "                unique_target_vals = temp_df[target].unique()\n",
        "                if len(unique_target_vals) == 2:\n",
        "                    target_map = {str(unique_target_vals[0]): 0, str(unique_target_vals[1]): 1}\n",
        "                    temp_df[target] = temp_df[target].map(target_map)\n",
        "                else:\n",
        "                    # Если больше 2 значений, берем два самых частых\n",
        "                    value_counts = temp_df[target].value_counts()\n",
        "                    top_values = value_counts.index[:2].tolist()\n",
        "                    target_map = {str(top_values[0]): 0, str(top_values[1]): 1}\n",
        "                    for val in unique_target_vals:\n",
        "                        if str(val) not in target_map:\n",
        "                            target_map[str(val)] = 0\n",
        "                    temp_df[target] = temp_df[target].map(target_map)\n",
        "            \n",
        "            # Удаляем строки с NaN в целевой переменной\n",
        "            temp_df = temp_df.dropna(subset=[target])\n",
        "            \n",
        "            if len(temp_df) < 10:  # Минимальное количество данных\n",
        "                return None\n",
        "            \n",
        "            # Для переменной: если это число, используем как есть\n",
        "            if pd.api.types.is_numeric_dtype(temp_df[variable]):\n",
        "                # Проверяем, что значения 0 и 1\n",
        "                unique_vals = temp_df[variable].dropna().unique()\n",
        "                unique_vals = [v for v in unique_vals if not pd.isna(v)]\n",
        "                \n",
        "                if len(unique_vals) == 2 and set(unique_vals).issubset({0, 1}):\n",
        "                    # Уже бинарная\n",
        "                    pass\n",
        "                else:\n",
        "                    # Преобразуем в бинарную (медиана как порог)\n",
        "                    median_val = temp_df[variable].median()\n",
        "                    temp_df[variable] = (temp_df[variable] > median_val).astype(int)\n",
        "            else:\n",
        "                # Если не числовая, преобразуем в бинарную\n",
        "                unique_vals = temp_df[variable].dropna().unique()\n",
        "                unique_vals = [str(v) for v in unique_vals if not pd.isna(v)]\n",
        "                \n",
        "                if len(unique_vals) >= 2:\n",
        "                    # Берем два самых частых значения\n",
        "                    value_counts = temp_df[variable].value_counts()\n",
        "                    top_values = value_counts.index[:2].tolist()\n",
        "                    \n",
        "                    # Создаем mapping\n",
        "                    val_map = {str(top_values[0]): 0, str(top_values[1]): 1}\n",
        "                    for val in unique_vals:\n",
        "                        if str(val) not in val_map:\n",
        "                            val_map[str(val)] = 0  # остальные относим к группе 0\n",
        "                    \n",
        "                    temp_df[variable] = temp_df[variable].astype(str).map(val_map)\n",
        "                else:\n",
        "                    return None\n",
        "            \n",
        "            # Удаляем строки с NaN в переменной\n",
        "            temp_df = temp_df.dropna(subset=[variable])\n",
        "            \n",
        "            if len(temp_df) < 10:  # Слишком мало данных\n",
        "                return None\n",
        "            \n",
        "            # Создаем таблицу сопряженности\n",
        "            contingency_table = pd.crosstab(temp_df[variable], temp_df[target])\n",
        "            \n",
        "            # Убедимся, что таблица 2x2\n",
        "            if contingency_table.shape != (2, 2):\n",
        "                # Если не 2x2, возможно нужно перекодировать\n",
        "                if len(contingency_table) == 2 and len(contingency_table.columns) == 2:\n",
        "                    pass  # Уже 2x2\n",
        "                else:\n",
        "                    return None\n",
        "            \n",
        "            # Извлекаем значения\n",
        "            try:\n",
        "                a = contingency_table.loc[1, 1] if 1 in contingency_table.index else 0\n",
        "                b = contingency_table.loc[1, 0] if 1 in contingency_table.index else 0\n",
        "                c = contingency_table.loc[0, 1] if 0 in contingency_table.index else 0\n",
        "                d = contingency_table.loc[0, 0] if 0 in contingency_table.index else 0\n",
        "            except KeyError:\n",
        "                return None\n",
        "            \n",
        "            # Расчет odds ratio с защитой от деления на 0\n",
        "            if b == 0 or c == 0:\n",
        "                odds_ratio = np.nan\n",
        "            else:\n",
        "                odds_ratio = (a * d) / (b * c)\n",
        "            \n",
        "            # Расчет rates\n",
        "            target_rate_1 = a / (a + b) if (a + b) > 0 else 0\n",
        "            target_rate_0 = c / (c + d) if (c + d) > 0 else 0\n",
        "            \n",
        "            return {\n",
        "                'variable': variable,\n",
        "                'odds_ratio': odds_ratio,\n",
        "                'target_rate_1': target_rate_1,\n",
        "                'target_rate_0': target_rate_0,\n",
        "                'count_1': int(a + b),\n",
        "                'count_0': int(c + d)\n",
        "            }\n",
        "        \n",
        "        except Exception as e:\n",
        "            return None\n",
        "    \n",
        "    # Анализ всех бинарных переменных\n",
        "    odds_ratios = []\n",
        "    print(f\"Расчет odds ratios для категориальных переменных...\")\n",
        "    for idx, var in enumerate(binary_vars):\n",
        "        if var in df.columns:\n",
        "            result = calculate_odds_ratio(df, var, target_var)\n",
        "            if result is not None and not pd.isna(result['odds_ratio']):\n",
        "                odds_ratios.append(result)\n",
        "    \n",
        "    # Создаем DataFrame с результатами\n",
        "    if odds_ratios:\n",
        "        odds_df = pd.DataFrame(odds_ratios)\n",
        "        \n",
        "        # Проверяем, есть ли столбец odds_ratio\n",
        "        if 'odds_ratio' in odds_df.columns and len(odds_df) > 0:\n",
        "            # Удаляем строки с NaN в odds_ratio\n",
        "            odds_df_clean = odds_df.dropna(subset=['odds_ratio'])\n",
        "            \n",
        "            if len(odds_df_clean) > 0:\n",
        "                odds_df = odds_df_clean.sort_values('odds_ratio', ascending=False)\n",
        "                \n",
        "                print(f\"\\nНайдено {len(odds_df)} переменных с корректными odds ratios\")\n",
        "                print(f\"\\nТоп-10 отношений шансов (Odds Ratios) для категориальных переменных:\")\n",
        "                print(odds_df[['variable', 'odds_ratio', 'target_rate_1', 'target_rate_0']].head(10).to_string())\n",
        "            else:\n",
        "                print(\"\\nНет корректных значений odds ratio для отображения\")\n",
        "                odds_df = pd.DataFrame()\n",
        "        else:\n",
        "            print(\"\\nНе удалось рассчитать odds ratios\")\n",
        "            odds_df = pd.DataFrame()\n",
        "    else:\n",
        "        print(\"\\nНе удалось рассчитать ни одного odds ratio\")\n",
        "        odds_df = pd.DataFrame()\n",
        "else:\n",
        "    print(\"\\n5.1 Целевая переменная не найдена, пропускаем анализ odds ratios\")\n",
        "    odds_df = pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.2 Визуализация отношения шансов\n",
        "if not odds_df.empty and len(odds_df) > 0 and 'odds_ratio' in odds_df.columns:\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    \n",
        "    # Берем топ 15 или меньше\n",
        "    n_to_show = min(15, len(odds_df))\n",
        "    top_odds = odds_df.head(n_to_show).sort_values('odds_ratio')\n",
        "    \n",
        "    # Создаем горизонтальную диаграмму\n",
        "    y_pos = np.arange(len(top_odds))\n",
        "    colors = ['red' if x < 1 else 'green' for x in top_odds['odds_ratio']]\n",
        "    \n",
        "    bars = plt.barh(y_pos, top_odds['odds_ratio'], color=colors, alpha=0.7)\n",
        "    \n",
        "    plt.yticks(y_pos, top_odds['variable'])\n",
        "    plt.xlabel('Отношение шансов (Odds Ratio)', fontweight='bold')\n",
        "    plt.title(f'Влияние категориальных переменных на {target_var} (Odds Ratio)', \n",
        "              fontsize=16, fontweight='bold')\n",
        "    plt.axvline(x=1, color='black', linestyle='--', linewidth=2)\n",
        "    plt.grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # Добавление аннотаций\n",
        "    for i, (bar, odds, rate1, rate0) in enumerate(zip(bars, top_odds['odds_ratio'], \n",
        "                                                       top_odds['target_rate_1'], \n",
        "                                                       top_odds['target_rate_0'])):\n",
        "        annotation = f'OR: {odds:.2f}\\nRate(1): {rate1:.1%}\\nRate(0): {rate0:.1%}'\n",
        "        x_pos = odds + 0.1 if odds < 10 else odds - 0.5\n",
        "        plt.text(x_pos, i, \n",
        "                 annotation, \n",
        "                 va='center', fontsize=8,\n",
        "                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('odds_ratios_binary_vars.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nПропускаем визуализацию odds ratios: недостаточно данных\")\n",
        "    \n",
        "    # Вместо этого создаем простую визуализацию целевой переменной по категориальным переменным\n",
        "    if target_candidates:\n",
        "        target_var = target_candidates[0]\n",
        "        plt.figure(figsize=(14, 10))\n",
        "        \n",
        "        # Берем первые 6 категориальных переменных\n",
        "        vars_to_plot = binary_vars[:6] if len(binary_vars) >= 6 else binary_vars\n",
        "        \n",
        "        if len(vars_to_plot) > 0:\n",
        "            n_cols = 3\n",
        "            n_rows = (len(vars_to_plot) + n_cols - 1) // n_cols\n",
        "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
        "            \n",
        "            if n_rows == 1:\n",
        "                axes = axes.reshape(1, -1)\n",
        "            axes = axes.flatten()\n",
        "            \n",
        "            for idx, var in enumerate(vars_to_plot):\n",
        "                if idx < len(axes) and var in df.columns:\n",
        "                    ax = axes[idx]\n",
        "                    \n",
        "                    try:\n",
        "                        # Создаем сводную таблицу\n",
        "                        temp_df = df[[var, target_var]].copy()\n",
        "                        temp_df = temp_df.dropna()\n",
        "                        \n",
        "                        if len(temp_df) > 0:\n",
        "                            # Для категориальных переменных\n",
        "                            if temp_df[var].nunique() <= 10:\n",
        "                                pivot = temp_df.groupby(var)[target_var].value_counts(normalize=True).unstack(fill_value=0)\n",
        "                                \n",
        "                                if len(pivot) > 0:\n",
        "                                    # Берем первый столбец (первое значение целевой переменной)\n",
        "                                    first_col = pivot.columns[0]\n",
        "                                    pivot[first_col].plot(kind='bar', ax=ax, color='skyblue', alpha=0.7)\n",
        "                                    ax.set_xlabel(var)\n",
        "                                    ax.set_ylabel(f'Процент {target_var}')\n",
        "                                    ax.set_title(f'{target_var} по {var}')\n",
        "                                    ax.grid(axis='y', alpha=0.3)\n",
        "                                    ax.tick_params(axis='x', rotation=45)\n",
        "                                    \n",
        "                                    # Добавить значения на столбцы\n",
        "                                    for i, val in enumerate(pivot[first_col]):\n",
        "                                        ax.text(i, val + 0.01, f'{val:.1%}', \n",
        "                                                ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
        "                                else:\n",
        "                                    ax.text(0.5, 0.5, 'Недостаточно данных', \n",
        "                                            transform=ax.transAxes, ha='center', va='center')\n",
        "                                    ax.set_title(f'{var}')\n",
        "                            else:\n",
        "                                ax.text(0.5, 0.5, 'Слишком много\\nуникальных значений', \n",
        "                                        transform=ax.transAxes, ha='center', va='center')\n",
        "                                ax.set_title(f'{var}')\n",
        "                        else:\n",
        "                            ax.text(0.5, 0.5, 'Нет данных', \n",
        "                                    transform=ax.transAxes, ha='center', va='center')\n",
        "                            ax.set_title(f'{var}')\n",
        "                    \n",
        "                    except Exception as e:\n",
        "                        ax.text(0.5, 0.5, f'Ошибка\\n{str(e)[:20]}', \n",
        "                                transform=ax.transAxes, ha='center', va='center')\n",
        "                        ax.set_title(f'{var}')\n",
        "            \n",
        "            # Скрыть пустые оси\n",
        "            for idx in range(len(vars_to_plot), len(axes)):\n",
        "                axes[idx].set_visible(False)\n",
        "            \n",
        "            plt.suptitle(f'Распределение {target_var} по категориальным переменным', fontsize=16, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('categorical_vars_target.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Нет категориальных переменных для визуализации\")\n",
        "    else:\n",
        "        print(\"Целевая переменная не найдена, пропускаем визуализацию\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. КЛАСТЕРНЫЙ АНАЛИЗ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. КЛАСТЕРНЫЙ АНАЛИЗ\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"6. КЛАСТЕРНЫЙ АНАЛИЗ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "# 6.1 Подготовка данных для кластеризации\n",
        "print(\"\\n6.1 Подготовка данных для кластеризации...\")\n",
        "\n",
        "# Выбираем ключевые переменные для кластеризации\n",
        "# Используем непрерывные переменные (первые 10-15 для кластеризации)\n",
        "if len(continuous_vars) > 0:\n",
        "    # Берем первые 15 непрерывных переменных, если их достаточно\n",
        "    cluster_features = continuous_vars[:15] if len(continuous_vars) >= 15 else continuous_vars\n",
        "    # Также добавляем порядковые переменные, если их немного\n",
        "    if len(ordinal_vars) > 0 and len(ordinal_vars) <= 5:\n",
        "        cluster_features = cluster_features + ordinal_vars\n",
        "else:\n",
        "    # Если нет непрерывных, используем порядковые\n",
        "    cluster_features = ordinal_vars[:15] if len(ordinal_vars) >= 15 else ordinal_vars\n",
        "\n",
        "# Проверяем, что все переменные существуют и числовые\n",
        "available_features = []\n",
        "for f in cluster_features:\n",
        "    if f in df.columns and pd.api.types.is_numeric_dtype(df[f]):\n",
        "        # Проверяем, что нет слишком много пропусков\n",
        "        if df[f].isnull().sum() / len(df) < 0.5:  # Меньше 50% пропусков\n",
        "            available_features.append(f)\n",
        "\n",
        "print(f\"Используем {len(available_features)} признаков для кластеризации\")\n",
        "if len(available_features) < 3:\n",
        "    print(\"ВНИМАНИЕ: Слишком мало признаков для кластеризации! Нужно минимум 3.\")\n",
        "else:\n",
        "    X_cluster = df[available_features].copy()\n",
        "    \n",
        "    # Удаляем строки с пропусками\n",
        "    X_cluster = X_cluster.dropna()\n",
        "    print(f\"Размер данных после удаления пропусков: {X_cluster.shape}\")\n",
        "    \n",
        "    if len(X_cluster) < 10:\n",
        "        print(\"ВНИМАНИЕ: Слишком мало данных для кластеризации!\")\n",
        "    else:\n",
        "        # Масштабирование данных\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X_cluster)\n",
        "        \n",
        "        print(\"Данные подготовлены и масштабированы\")\n",
        "        # 6.2 Определение оптимального числа кластеров\n",
        "        if len(X_cluster) >= 10:\n",
        "            print(\"\\n6.2 Определение оптимального числа кластеров...\")\n",
        "            \n",
        "            # Метод локтя и силуэтный анализ\n",
        "            wcss = []  # Within-cluster sum of squares\n",
        "            silhouette_scores = []\n",
        "            db_scores = []  # Davies-Bouldin scores\n",
        "            max_k = min(10, len(X_cluster) // 2)  # Не больше половины размера данных\n",
        "            K_range = range(2, max_k + 1)\n",
        "            \n",
        "            for k in K_range:\n",
        "                kmeans = KMeans(n_clusters=k, random_state=42, n_init=20, max_iter=300)\n",
        "                kmeans.fit(X_scaled)\n",
        "                wcss.append(kmeans.inertia_)\n",
        "                \n",
        "                if k > 1:\n",
        "                    silhouette_avg = silhouette_score(X_scaled, kmeans.labels_)\n",
        "                    silhouette_scores.append(silhouette_avg)\n",
        "                    \n",
        "                    db_score = davies_bouldin_score(X_scaled, kmeans.labels_)\n",
        "                    db_scores.append(db_score)\n",
        "                \n",
        "                print(f\"k={k}: WCSS={kmeans.inertia_:.2f}, \", end=\"\")\n",
        "                if k > 1:\n",
        "                    print(f\"Silhouette={silhouette_avg:.3f}, DB={db_score:.3f}\")\n",
        "                else:\n",
        "                    print()\n",
        "            \n",
        "            # Визуализация методов определения k\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "            \n",
        "            # Метод локтя\n",
        "            axes[0].plot(K_range, wcss, 'bo-', linewidth=2, markersize=8)\n",
        "            axes[0].set_xlabel('Количество кластеров (k)', fontweight='bold')\n",
        "            axes[0].set_ylabel('WCSS (Within-Cluster Sum of Squares)', fontweight='bold')\n",
        "            axes[0].set_title('Метод локтя', fontweight='bold')\n",
        "            axes[0].grid(True, alpha=0.3)\n",
        "            \n",
        "            # Силуэтный анализ\n",
        "            if len(silhouette_scores) > 0:\n",
        "                axes[1].plot(range(2, max_k + 1), silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
        "                axes[1].set_xlabel('Количество кластеров (k)', fontweight='bold')\n",
        "                axes[1].set_ylabel('Средний силуэтный коэффициент', fontweight='bold')\n",
        "                axes[1].set_title('Силуэтный анализ', fontweight='bold')\n",
        "                axes[1].grid(True, alpha=0.3)\n",
        "            \n",
        "            # Davies-Bouldin Index\n",
        "            if len(db_scores) > 0:\n",
        "                axes[2].plot(range(2, max_k + 1), db_scores, 'go-', linewidth=2, markersize=8)\n",
        "                axes[2].set_xlabel('Количество кластеров (k)', fontweight='bold')\n",
        "                axes[2].set_ylabel('Davies-Bouldin Index', fontweight='bold')\n",
        "                axes[2].set_title('Davies-Bouldin Index (меньше = лучше)', fontweight='bold')\n",
        "                axes[2].grid(True, alpha=0.3)\n",
        "            \n",
        "            plt.suptitle('Определение оптимального числа кластеров', fontsize=16, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('optimal_clusters_determination.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            \n",
        "            # Выбираем оптимальное k (по силуэтному коэффициенту)\n",
        "            if len(silhouette_scores) > 0:\n",
        "                optimal_k = range(2, max_k + 1)[np.argmax(silhouette_scores)]\n",
        "                print(f\"\\nОптимальное число кластеров (по силуэтному коэффициенту): {optimal_k}\")\n",
        "            else:\n",
        "                optimal_k = 3  # Значение по умолчанию\n",
        "                print(f\"\\nИспользуем значение по умолчанию: k={optimal_k}\")\n",
        "            # 6.3 Применение K-means с оптимальным k\n",
        "            print(f\"\\n6.3 Применение K-means с k={optimal_k}...\")\n",
        "            kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=20, max_iter=300)\n",
        "            cluster_labels = kmeans.fit_predict(X_scaled)\n",
        "            \n",
        "            # Добавляем кластеры в исходный датасет (только для строк без пропусков)\n",
        "            # Создаем новый столбец Cluster\n",
        "            df['Cluster'] = np.nan\n",
        "            df.loc[X_cluster.index, 'Cluster'] = cluster_labels\n",
        "            \n",
        "            print(f\"Кластеризация завершена. Создано {optimal_k} кластеров.\")\n",
        "            # 6.4 Снижение размерности для визуализации\n",
        "            print(\"\\n6.4 Снижение размерности и визуализация кластеров...\")\n",
        "            \n",
        "            # PCA для визуализации\n",
        "            pca = PCA(n_components=2)\n",
        "            X_pca = pca.fit_transform(X_scaled)\n",
        "            \n",
        "            print(f\"Объясненная дисперсия PCA: {pca.explained_variance_ratio_.sum():.3f}\")\n",
        "            print(f\"PC1: {pca.explained_variance_ratio_[0]:.3f}\")\n",
        "            print(f\"PC2: {pca.explained_variance_ratio_[1]:.3f}\")\n",
        "            \n",
        "            # t-SNE для нелинейного снижения размерности\n",
        "            print(\"Применение t-SNE...\")\n",
        "            try:\n",
        "                # Для новых версий scikit-learn (>= 0.24)\n",
        "                tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X_scaled)-1), \n",
        "                           max_iter=1000, learning_rate=200)\n",
        "            except TypeError:\n",
        "                try:\n",
        "                    # Для версий 0.19-0.23\n",
        "                    tsne = TSNE(n_components=2, random_state=42, \n",
        "                               perplexity=min(30, len(X_scaled)-1), \n",
        "                               n_iter=1000, learning_rate=200)\n",
        "                except TypeError:\n",
        "                    # Минимальная конфигурация\n",
        "                    tsne = TSNE(n_components=2, random_state=42, \n",
        "                               perplexity=min(30, len(X_scaled)-1))\n",
        "                    print(\"Используем упрощенную конфигурацию t-SNE\")\n",
        "            \n",
        "            try:\n",
        "                X_tsne = tsne.fit_transform(X_scaled)\n",
        "                print(\"t-SNE успешно выполнен\")\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка при выполнении t-SNE: {e}\")\n",
        "                print(\"Используем PCA для визуализации вместо t-SNE\")\n",
        "                X_tsne = X_pca  # Используем PCA как запасной вариант\n",
        "            \n",
        "            # Визуализация кластеров\n",
        "            n_plots = 2\n",
        "            if target_candidates:\n",
        "                n_plots = 3\n",
        "            \n",
        "            fig, axes = plt.subplots(1, n_plots, figsize=(6*n_plots, 6))\n",
        "            if n_plots == 1:\n",
        "                axes = [axes]\n",
        "            \n",
        "            # PCA визуализация\n",
        "            scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
        "                                      c=cluster_labels, cmap='tab10', alpha=0.7, s=50)\n",
        "            axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
        "            axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
        "            axes[0].set_title('Кластеры в PCA пространстве', fontweight='bold')\n",
        "            plt.colorbar(scatter1, ax=axes[0], label='Кластер')\n",
        "            \n",
        "            # t-SNE визуализация\n",
        "            scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
        "                                      c=cluster_labels, cmap='tab10', alpha=0.7, s=50)\n",
        "            axes[1].set_xlabel('t-SNE Component 1')\n",
        "            axes[1].set_ylabel('t-SNE Component 2')\n",
        "            axes[1].set_title('Кластеры в t-SNE пространстве', fontweight='bold')\n",
        "            plt.colorbar(scatter2, ax=axes[1], label='Кластер')\n",
        "            \n",
        "            # Распределение целевой переменной по кластерам (если есть)\n",
        "            if target_candidates and n_plots == 3:\n",
        "                target_var = target_candidates[0]\n",
        "                # Берем только строки с кластерами\n",
        "                df_clustered = df[df['Cluster'].notna()].copy()\n",
        "                if len(df_clustered) > 0:\n",
        "                    cluster_target = pd.crosstab(df_clustered['Cluster'], df_clustered[target_var], normalize='index') * 100\n",
        "                    cluster_target.plot(kind='bar', stacked=True, ax=axes[2], \n",
        "                                      color=sns.color_palette(\"husl\", len(cluster_target.columns)))\n",
        "                    axes[2].set_xlabel('Кластер', fontweight='bold')\n",
        "                    axes[2].set_ylabel('Процент (%)', fontweight='bold')\n",
        "                    axes[2].set_title(f'Распределение {target_var} по кластерам', fontweight='bold')\n",
        "                    axes[2].legend(title=target_var)\n",
        "                    axes[2].grid(axis='y', alpha=0.3)\n",
        "            \n",
        "            plt.suptitle(f'Кластерный анализ (k={optimal_k})', fontsize=16, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('clustering_results.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            # 6.5 Анализ характеристик кластеров\n",
        "            print(\"\\n6.5 Характеристики кластеров:\")\n",
        "            \n",
        "            # Берем только строки с кластерами\n",
        "            df_clustered = df[df['Cluster'].notna()].copy()\n",
        "            \n",
        "            # Средние значения по кластерам\n",
        "            cluster_profiles = df_clustered.groupby('Cluster')[available_features].mean()\n",
        "            print(\"\\nСредние значения по кластерам для ключевых переменных:\")\n",
        "            print(cluster_profiles.round(2))\n",
        "            \n",
        "            # Размеры кластеров\n",
        "            print(\"\\nРазмеры кластеров:\")\n",
        "            cluster_sizes = df_clustered['Cluster'].value_counts().sort_index()\n",
        "            print(cluster_sizes)\n",
        "            print(f\"\\nПроцентное распределение:\")\n",
        "            for cluster, size in cluster_sizes.items():\n",
        "                percentage = (size / len(df_clustered)) * 100\n",
        "                print(f\"  Кластер {int(cluster)}: {size} ({percentage:.1f}%)\")\n",
        "            \n",
        "            # Визуализация профилей кластеров (Heatmap)\n",
        "            if len(cluster_profiles) > 0:\n",
        "                plt.figure(figsize=(14, 8))\n",
        "                sns.heatmap(cluster_profiles.T, annot=True, cmap='YlOrRd', \n",
        "                            fmt='.1f', linewidths=0.5, cbar_kws={'label': 'Среднее значение'})\n",
        "                plt.title('Профили кластеров (средние значения признаков)', fontsize=16, fontweight='bold')\n",
        "                plt.xlabel('Кластер')\n",
        "                plt.ylabel('Признак')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig('cluster_profiles_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "                plt.show()\n",
        "            else:\n",
        "                print(\"Не удалось создать профили кластеров\")\n",
        "        else:\n",
        "            print(\"Недостаточно данных для кластеризации\")\n",
        "    else:\n",
        "        print(\"Недостаточно признаков для кластеризации\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. PCA АНАЛИЗ И СНИЖЕНИЕ РАЗМЕРНОСТИ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. СНИЖЕНИЕ РАЗМЕРНОСТИ И ВИЗУАЛИЗАЦИЯ\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"7. PCA АНАЛИЗ И СНИЖЕНИЕ РАЗМЕРНОСТИ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 7.1 Полный PCA анализ\n",
        "print(\"\\n7.1 PCA анализ всех числовых переменных...\")\n",
        "\n",
        "# Используем все числовые переменные для PCA\n",
        "all_numeric_vars = continuous_vars + ordinal_vars\n",
        "\n",
        "# Фильтруем только существующие и числовые переменные\n",
        "valid_numeric_vars = []\n",
        "for var in all_numeric_vars:\n",
        "    if var in df.columns and pd.api.types.is_numeric_dtype(df[var]):\n",
        "        # Проверяем, что нет слишком много пропусков\n",
        "        if df[var].isnull().sum() / len(df) < 0.5:  # Меньше 50% пропусков\n",
        "            valid_numeric_vars.append(var)\n",
        "\n",
        "print(f\"Используем {len(valid_numeric_vars)} числовых переменных для PCA\")\n",
        "\n",
        "if len(valid_numeric_vars) < 2:\n",
        "    print(\"ВНИМАНИЕ: Недостаточно переменных для PCA анализа! Нужно минимум 2.\")\n",
        "else:\n",
        "    X_pca_full = df[valid_numeric_vars].copy()\n",
        "    \n",
        "    # Удаляем строки с пропусками\n",
        "    X_pca_full = X_pca_full.dropna()\n",
        "    print(f\"Размер данных после удаления пропусков: {X_pca_full.shape}\")\n",
        "    \n",
        "    if len(X_pca_full) < 2:\n",
        "        print(\"ВНИМАНИЕ: Недостаточно данных для PCA анализа!\")\n",
        "    else:\n",
        "        # Масштабирование\n",
        "        scaler_pca = StandardScaler()\n",
        "        X_pca_scaled = scaler_pca.fit_transform(X_pca_full)\n",
        "        \n",
        "        # PCA со всеми компонентами\n",
        "        pca_full = PCA()\n",
        "        X_pca_transformed = pca_full.fit_transform(X_pca_scaled)\n",
        "        \n",
        "        # Анализ объясненной дисперсии\n",
        "        explained_variance = pca_full.explained_variance_ratio_\n",
        "        cumulative_variance = np.cumsum(explained_variance)\n",
        "        \n",
        "        print(f\"Создано {len(explained_variance)} главных компонент\")\n",
        "                # 7.2 Визуализация объясненной дисперсии\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "        \n",
        "        # Scree plot\n",
        "        axes[0].plot(range(1, len(explained_variance) + 1), explained_variance, 'bo-', linewidth=2)\n",
        "        axes[0].set_xlabel('Номер главной компоненты', fontweight='bold')\n",
        "        axes[0].set_ylabel('Объясненная дисперсия', fontweight='bold')\n",
        "        axes[0].set_title('Scree Plot (объясненная дисперсия)', fontweight='bold')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Кумулятивная дисперсия\n",
        "        axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'ro-', linewidth=2)\n",
        "        axes[1].axhline(y=0.8, color='g', linestyle='--', label='80% variance')\n",
        "        axes[1].axhline(y=0.9, color='b', linestyle='--', label='90% variance')\n",
        "        axes[1].axhline(y=0.95, color='purple', linestyle='--', label='95% variance')\n",
        "        axes[1].set_xlabel('Количество главных компонент', fontweight='bold')\n",
        "        axes[1].set_ylabel('Кумулятивная объясненная дисперсия', fontweight='bold')\n",
        "        axes[1].set_title('Кумулятивная объясненная дисперсия', fontweight='bold')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.suptitle('PCA анализ: объясненная дисперсия', fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('pca_variance_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        # Определение числа компонент для 80%, 90%, 95% дисперсии\n",
        "        n_components_80 = np.where(cumulative_variance >= 0.8)[0]\n",
        "        n_components_90 = np.where(cumulative_variance >= 0.9)[0]\n",
        "        n_components_95 = np.where(cumulative_variance >= 0.95)[0]\n",
        "        \n",
        "        if len(n_components_80) > 0:\n",
        "            n_comp_80 = n_components_80[0] + 1\n",
        "            print(f\"\\nДля объяснения 80% дисперсии нужно {n_comp_80} компонент\")\n",
        "        else:\n",
        "            print(f\"\\nДля объяснения 80% дисперсии нужно более {len(explained_variance)} компонент\")\n",
        "        \n",
        "        if len(n_components_90) > 0:\n",
        "            n_comp_90 = n_components_90[0] + 1\n",
        "            print(f\"Для объяснения 90% дисперсии нужно {n_comp_90} компонент\")\n",
        "        else:\n",
        "            print(f\"Для объяснения 90% дисперсии нужно более {len(explained_variance)} компонент\")\n",
        "        \n",
        "        if len(n_components_95) > 0:\n",
        "            n_comp_95 = n_components_95[0] + 1\n",
        "            print(f\"Для объяснения 95% дисперсии нужно {n_comp_95} компонент\")\n",
        "        else:\n",
        "            print(f\"Для объяснения 95% дисперсии нужно более {len(explained_variance)} компонент\")\n",
        "                    # 7.3 Анализ нагрузок на первые две компоненты\n",
        "        print(\"\\n7.3 Анализ нагрузок на главные компоненты...\")\n",
        "        \n",
        "        # Создаем DataFrame с нагрузками\n",
        "        loadings = pd.DataFrame(\n",
        "            pca_full.components_[:2].T,\n",
        "            columns=['PC1', 'PC2'],\n",
        "            index=valid_numeric_vars\n",
        "        )\n",
        "        \n",
        "        # Сортируем по абсолютной величине нагрузок\n",
        "        print(\"\\nТоп-10 переменных с наибольшими нагрузками на PC1:\")\n",
        "        print(loadings['PC1'].abs().sort_values(ascending=False).head(10))\n",
        "        \n",
        "        print(\"\\nТоп-10 переменных с наибольшими нагрузками на PC2:\")\n",
        "        print(loadings['PC2'].abs().sort_values(ascending=False).head(10))\n",
        "        \n",
        "        # Визуализация нагрузок\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "        \n",
        "        # Нагрузки на PC1\n",
        "        top_pc1 = loadings['PC1'].abs().sort_values(ascending=False).head(15)\n",
        "        colors_pc1 = ['red' if x < 0 else 'green' for x in loadings.loc[top_pc1.index, 'PC1']]\n",
        "        axes[0].barh(range(len(top_pc1)), loadings.loc[top_pc1.index, 'PC1'], color=colors_pc1)\n",
        "        axes[0].set_yticks(range(len(top_pc1)))\n",
        "        axes[0].set_yticklabels(top_pc1.index)\n",
        "        axes[0].set_xlabel('Нагрузка', fontweight='bold')\n",
        "        axes[0].set_title(f'Нагрузки на PC1 ({explained_variance[0]*100:.1f}% variance)', fontweight='bold')\n",
        "        axes[0].axvline(x=0, color='black', linewidth=0.5)\n",
        "        axes[0].grid(axis='x', alpha=0.3)\n",
        "        \n",
        "        # Нагрузки на PC2\n",
        "        top_pc2 = loadings['PC2'].abs().sort_values(ascending=False).head(15)\n",
        "        colors_pc2 = ['red' if x < 0 else 'green' for x in loadings.loc[top_pc2.index, 'PC2']]\n",
        "        axes[1].barh(range(len(top_pc2)), loadings.loc[top_pc2.index, 'PC2'], color=colors_pc2)\n",
        "        axes[1].set_yticks(range(len(top_pc2)))\n",
        "        axes[1].set_yticklabels(top_pc2.index)\n",
        "        axes[1].set_xlabel('Нагрузка', fontweight='bold')\n",
        "        axes[1].set_title(f'Нагрузки на PC2 ({explained_variance[1]*100:.1f}% variance)', fontweight='bold')\n",
        "        axes[1].axvline(x=0, color='black', linewidth=0.5)\n",
        "        axes[1].grid(axis='x', alpha=0.3)\n",
        "        \n",
        "        plt.suptitle('Нагрузки переменных на главные компоненты', fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('pca_loadings_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        # 7.4 Biplot (комбинированная визуализация)\n",
        "        print(\"\\n7.4 Biplot визуализация...\")\n",
        "        \n",
        "        # Создаем biplot\n",
        "        plt.figure(figsize=(14, 10))\n",
        "        \n",
        "        # Определяем цветовую схему для точек\n",
        "        if target_candidates and 'target_var_numeric' in locals() and target_var_numeric and target_var_numeric in df_numeric.columns:\n",
        "            # Используем целевую переменную для раскраски\n",
        "            # Берем только строки, которые есть в X_pca_full\n",
        "            target_values_for_plot = df_numeric.loc[X_pca_full.index, target_var_numeric]\n",
        "            scatter = plt.scatter(X_pca_transformed[:, 0], X_pca_transformed[:, 1], \n",
        "                                 c=target_values_for_plot, cmap='coolwarm', \n",
        "                                 alpha=0.6, s=50, edgecolor='k', linewidth=0.5)\n",
        "            plt.colorbar(scatter, label=f'{target_var} (numeric)')\n",
        "        elif 'Cluster' in df.columns:\n",
        "            # Используем кластеры для раскраски, если они есть\n",
        "            cluster_values_for_plot = df.loc[X_pca_full.index, 'Cluster']\n",
        "            cluster_values_for_plot = cluster_values_for_plot.dropna()\n",
        "            if len(cluster_values_for_plot) > 0:\n",
        "                # Берем только строки с кластерами\n",
        "                mask = cluster_values_for_plot.notna()\n",
        "                scatter = plt.scatter(X_pca_transformed[mask, 0], X_pca_transformed[mask, 1], \n",
        "                                     c=cluster_values_for_plot[mask], cmap='tab10', \n",
        "                                     alpha=0.6, s=50, edgecolor='k', linewidth=0.5)\n",
        "                plt.colorbar(scatter, label='Cluster')\n",
        "            else:\n",
        "                scatter = plt.scatter(X_pca_transformed[:, 0], X_pca_transformed[:, 1], \n",
        "                                     alpha=0.6, s=50, edgecolor='k', linewidth=0.5, color='blue')\n",
        "        else:\n",
        "            # Без раскраски\n",
        "            scatter = plt.scatter(X_pca_transformed[:, 0], X_pca_transformed[:, 1], \n",
        "                                 alpha=0.6, s=50, edgecolor='k', linewidth=0.5, color='blue')\n",
        "        \n",
        "        # Векторы нагрузок (показываем только топ-10 для читаемости)\n",
        "        scale_factor = 5  # Масштаб для векторов\n",
        "        top_features = loadings['PC1'].abs().sort_values(ascending=False).head(10).index\n",
        "        \n",
        "        for feature in top_features:\n",
        "            plt.arrow(0, 0, loadings.loc[feature, 'PC1'] * scale_factor, \n",
        "                      loadings.loc[feature, 'PC2'] * scale_factor,\n",
        "                      color='black', alpha=0.5, head_width=0.05)\n",
        "            plt.text(loadings.loc[feature, 'PC1'] * scale_factor * 1.15,\n",
        "                     loadings.loc[feature, 'PC2'] * scale_factor * 1.15,\n",
        "                     feature, color='darkblue', fontsize=9, fontweight='bold')\n",
        "        \n",
        "        plt.xlabel(f'PC1 ({explained_variance[0]*100:.1f}% variance)', fontweight='bold')\n",
        "        plt.ylabel(f'PC2 ({explained_variance[1]*100:.1f}% variance)', fontweight='bold')\n",
        "        plt.title('Biplot: PCA проекция с нагрузками переменных', fontsize=16, fontweight='bold')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
        "        plt.axvline(x=0, color='k', linestyle='--', linewidth=0.5)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('pca_biplot.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"PCA анализ завершен!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. ПОСТРОЕНИЕ ПРОГНОЗИРУЮЩЕЙ МОДЕЛИ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. ПОСТРОЕНИЕ ПРОГНОЗИРУЮЩЕЙ МОДЕЛИ\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"8. ПОСТРОЕНИЕ ПРОГНОЗИРУЮЩЕЙ МОДЕЛИ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, \n",
        "                           roc_curve, auc, roc_auc_score, precision_recall_curve,\n",
        "                           f1_score, precision_score, recall_score)\n",
        "\n",
        "# Проверка данных перед моделированием\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ПРОВЕРКА ДАННЫХ ПЕРЕД МОДЕЛИРОВАНИЕМ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"Общий размер датасета: {df.shape}\")\n",
        "print(f\"Количество строк: {df.shape[0]}\")\n",
        "print(f\"Количество столбцов: {df.shape[1]}\")\n",
        "\n",
        "# Проверяем целевую переменную (используем автоматически найденную)\n",
        "if target_candidates and len(target_candidates) > 0:\n",
        "    target_var = target_candidates[0]\n",
        "    print(f\"\\nАнализ целевой переменной {target_var}:\")\n",
        "    print(f\"  Тип данных: {df[target_var].dtype}\")\n",
        "    print(f\"  Уникальные значения: {df[target_var].unique()}\")\n",
        "    \n",
        "    # Подсчет распределения\n",
        "    target_counts = df[target_var].value_counts()\n",
        "    for val, count in target_counts.items():\n",
        "        percentage = count / len(df) * 100\n",
        "        print(f\"  '{val}': {count} ({percentage:.1f}%)\")\n",
        "    \n",
        "    # Проверяем на NaN\n",
        "    nan_count = df[target_var].isna().sum()\n",
        "    if nan_count > 0:\n",
        "        print(f\"  ВНИМАНИЕ: Найдено {nan_count} NaN значений в {target_var}!\")\n",
        "        \n",
        "        # Заполняем наиболее частым значением\n",
        "        mode_val = df[target_var].mode()[0] if not df[target_var].mode().empty else df[target_var].value_counts().index[0]\n",
        "        print(f\"  Заполняем NaN значением '{mode_val}'...\")\n",
        "        df[target_var].fillna(mode_val, inplace=True)\n",
        "    \n",
        "    # Приводим к стандартному виду (если строковая)\n",
        "    if df[target_var].dtype == 'object':\n",
        "        print(\"  Приводим к стандартному виду...\")\n",
        "        df[target_var] = df[target_var].astype(str).str.strip().str.title()\n",
        "else:\n",
        "    print(\"\\nВНИМАНИЕ: Целевая переменная не найдена автоматически!\")\n",
        "    print(\"Для построения модели нужна целевая переменная.\")\n",
        "    target_var = None\n",
        "\n",
        "print(\"\\nПроверка числовых признаков на проблемы:\")\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "problem_cols = []\n",
        "\n",
        "for col in numeric_cols:\n",
        "    nan_count = df[col].isna().sum()\n",
        "    inf_count = np.isinf(df[col]).sum()\n",
        "    \n",
        "    if nan_count > 0 or inf_count > 0:\n",
        "        problem_cols.append((col, nan_count, inf_count))\n",
        "\n",
        "if len(problem_cols) > 0:\n",
        "    print(f\"  Найдено {len(problem_cols)} проблемных колонок:\")\n",
        "    for col, nan_count, inf_count in problem_cols[:10]:  # Показываем первые 10\n",
        "        print(f\"    {col}: {nan_count} NaN, {inf_count} inf/-inf\")\n",
        "    \n",
        "    print(\"\\n  Исправление проблемных колонок...\")\n",
        "    for col in numeric_cols:\n",
        "        # Заменяем inf на NaN\n",
        "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
        "        \n",
        "        # Заполняем NaN медианой\n",
        "        if df[col].isna().sum() > 0:\n",
        "            median_val = df[col].median()\n",
        "            if pd.notna(median_val):\n",
        "                df[col].fillna(median_val, inplace=True)\n",
        "            else:\n",
        "                df[col].fillna(0, inplace=True)\n",
        "    \n",
        "    print(\"  Проблемы исправлены!\")\n",
        "else:\n",
        "    print(\"  Проблемных колонок не найдено\")\n",
        "\n",
        "print(\"\\nПроверка категориальных признаков:\")\n",
        "categorical_cols_check = df.select_dtypes(include=['object']).columns\n",
        "if len(categorical_cols_check) > 0:\n",
        "    print(f\"  Найдено {len(categorical_cols_check)} категориальных колонок:\")\n",
        "    for col in categorical_cols_check[:5]:  # Показываем первые 5\n",
        "        print(f\"    {col}: {df[col].dtype}, уникальных значений: {df[col].nunique()}\")\n",
        "else:\n",
        "    print(\"  Категориальных колонок не найдено\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8.1 Подготовка данных для моделирования\n",
        "if target_var and target_var in df.columns:\n",
        "    print(\"\\n8.1 Подготовка данных для моделирования...\")\n",
        "    \n",
        "    # Создаем бинарную целевую переменную\n",
        "    print(f\"Преобразование целевой переменной {target_var}...\")\n",
        "    print(f\"Уникальные значения {target_var}: {df[target_var].unique()}\")\n",
        "    \n",
        "    # Приводим к стандартному виду (если строковая)\n",
        "    if df[target_var].dtype == 'object':\n",
        "        df[target_var] = df[target_var].astype(str).str.strip().str.title()\n",
        "    \n",
        "    # Создаем бинарную целевую переменную\n",
        "    # Определяем уникальные значения\n",
        "    unique_vals = df[target_var].dropna().unique()\n",
        "    \n",
        "    if len(unique_vals) == 2:\n",
        "        # Бинарная переменная - создаем маппинг\n",
        "        sorted_vals = sorted([str(v) for v in unique_vals])\n",
        "        target_map = {sorted_vals[0]: 0, sorted_vals[1]: 1}\n",
        "        # Также добавляем варианты написания\n",
        "        for val in unique_vals:\n",
        "            val_str = str(val).strip().title()\n",
        "            if val_str not in target_map:\n",
        "                # Определяем к какому классу отнести\n",
        "                if any(keyword in val_str.lower() for keyword in ['yes', '1', 'true', 'positive']):\n",
        "                    target_map[val_str] = 1\n",
        "                else:\n",
        "                    target_map[val_str] = 0\n",
        "        \n",
        "        y = df[target_var].astype(str).map(target_map)\n",
        "    elif len(unique_vals) > 2:\n",
        "        # Многоклассовая - берем два самых частых класса\n",
        "        print(f\"  ВНИМАНИЕ: {target_var} имеет {len(unique_vals)} классов. Берем два самых частых.\")\n",
        "        value_counts = df[target_var].value_counts()\n",
        "        top_two = value_counts.head(2).index.tolist()\n",
        "        target_map = {str(top_two[0]): 0, str(top_two[1]): 1}\n",
        "        # Остальные относим к классу 0\n",
        "        for val in unique_vals:\n",
        "            if str(val) not in target_map:\n",
        "                target_map[str(val)] = 0\n",
        "        y = df[target_var].astype(str).map(target_map)\n",
        "    else:\n",
        "        # Если числовая, проверяем бинарность\n",
        "        if pd.api.types.is_numeric_dtype(df[target_var]):\n",
        "            unique_numeric = df[target_var].dropna().unique()\n",
        "            if len(unique_numeric) == 2:\n",
        "                # Уже бинарная числовая\n",
        "                y = df[target_var].copy()\n",
        "            else:\n",
        "                # Преобразуем в бинарную (медиана как порог)\n",
        "                median_val = df[target_var].median()\n",
        "                y = (df[target_var] > median_val).astype(int)\n",
        "                print(f\"  Преобразуем в бинарную используя медиану ({median_val}) как порог\")\n",
        "        else:\n",
        "            print(f\"  ОШИБКА: Не удалось преобразовать {target_var} в бинарную переменную\")\n",
        "            y = None\n",
        "    \n",
        "    if y is not None:\n",
        "        # Проверяем результат\n",
        "        print(f\"\\nРаспределение целевой переменной:\")\n",
        "        print(f\"  Всего записей: {len(y)}\")\n",
        "        print(f\"  Класс 1: {(y == 1).sum()} ({(y == 1).mean()*100:.1f}%)\")\n",
        "        print(f\"  Класс 0: {(y == 0).sum()} ({(y == 0).mean()*100:.1f}%)\")\n",
        "        print(f\"  NaN значений: {y.isna().sum()}\")\n",
        "        \n",
        "        # Удаляем строки с NaN в целевой переменной\n",
        "        if y.isna().sum() > 0:\n",
        "            print(f\"\\nУдаляем {y.isna().sum()} строк с NaN в целевой переменной...\")\n",
        "            valid_indices = y.dropna().index\n",
        "            df = df.loc[valid_indices].copy()\n",
        "            y = y.loc[valid_indices].copy()\n",
        "            \n",
        "            print(f\"  Осталось записей: {len(y)}\")\n",
        "            print(f\"  Класс 1: {(y == 1).sum()} ({(y == 1).mean()*100:.1f}%)\")\n",
        "            print(f\"  Класс 0: {(y == 0).sum()} ({(y == 0).mean()*100:.1f}%)\")\n",
        "        \n",
        "        # Проверяем баланс классов\n",
        "        if (y == 1).sum() < 2 or (y == 0).sum() < 2:\n",
        "            print(\"\\nВНИМАНИЕ: Слишком мало примеров одного из классов для моделирования!\")\n",
        "            y = None\n",
        "else:\n",
        "    print(\"\\n8.1 Пропускаем подготовку данных: целевая переменная не найдена\")\n",
        "    y = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Подготовка признаков\n",
        "if y is not None and target_var:\n",
        "    print(\"\\nПодготовка матрицы признаков...\")\n",
        "    \n",
        "    # Список колонок для исключения\n",
        "    cols_to_drop = [target_var, 'Cluster']\n",
        "    if 'target_var_numeric' in locals() and target_var_numeric and target_var_numeric in df.columns:\n",
        "        cols_to_drop.append(target_var_numeric)\n",
        "    \n",
        "    X = df.drop(cols_to_drop, axis=1, errors='ignore')\n",
        "    \n",
        "    # Проверяем наличие нечисловых колонок\n",
        "    non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns\n",
        "    if len(non_numeric_cols) > 0:\n",
        "        print(f\"Найдены нечисловые колонки ({len(non_numeric_cols)}): {list(non_numeric_cols)[:10]}\")\n",
        "        X = pd.get_dummies(X, drop_first=True)\n",
        "        print(f\"После one-hot encoding: {X.shape[1]} признаков\")\n",
        "    else:\n",
        "        print(\"Все колонки уже числовые\")\n",
        "    \n",
        "    print(f\"\\nРазмер матрицы признаков: {X.shape}\")\n",
        "    print(f\"Баланс классов: {y.value_counts().to_dict()}\")\n",
        "    \n",
        "    # Проверяем на NaN в признаках\n",
        "    nan_in_X = X.isna().sum().sum()\n",
        "    if nan_in_X > 0:\n",
        "        print(f\"\\nПредупреждение: в матрице признаков найдено {nan_in_X} NaN значений\")\n",
        "        print(\"Заполняем NaN медианами по столбцам...\")\n",
        "        for col in X.columns:\n",
        "            if X[col].isna().sum() > 0:\n",
        "                median_val = X[col].median()\n",
        "                if pd.notna(median_val):\n",
        "                    X[col].fillna(median_val, inplace=True)\n",
        "                else:\n",
        "                    X[col].fillna(0, inplace=True)\n",
        "    \n",
        "    # Проверяем на бесконечные значения\n",
        "    inf_in_X = np.isinf(X.values).sum()\n",
        "    if inf_in_X > 0:\n",
        "        print(f\"\\nПредупреждение: в матрице признаков найдено {inf_in_X} inf/-inf значений\")\n",
        "        print(\"Заменяем inf/-inf на NaN, затем заполняем медианами...\")\n",
        "        X = X.replace([np.inf, -np.inf], np.nan)\n",
        "        for col in X.columns:\n",
        "            if X[col].isna().sum() > 0:\n",
        "                median_val = X[col].median()\n",
        "                if pd.notna(median_val):\n",
        "                    X[col].fillna(median_val, inplace=True)\n",
        "                else:\n",
        "                    X[col].fillna(0, inplace=True)\n",
        "    \n",
        "    # Разделение данных\n",
        "    print(\"\\nРазделение данных на тренировочную и тестовую выборки...\")\n",
        "    try:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.3, random_state=42, stratify=y\n",
        "        )\n",
        "        \n",
        "        print(f\"Размер тренировочной выборки: {X_train.shape}\")\n",
        "        print(f\"Размер тестовой выборки: {X_test.shape}\")\n",
        "        print(f\"Класс 1 в тренировочной выборке: {y_train.mean():.3f} ({y_train.sum()}/{len(y_train)})\")\n",
        "        print(f\"Класс 1 в тестовой выборке: {y_test.mean():.3f} ({y_test.sum()}/{len(y_test)})\")\n",
        "        \n",
        "    except ValueError as e:\n",
        "        print(f\"Ошибка при разделении данных: {e}\")\n",
        "        print(\"Пробуем разделение без stratify...\")\n",
        "        \n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.3, random_state=42\n",
        "        )\n",
        "        \n",
        "        print(f\"Размер тренировочной выборки: {X_train.shape}\")\n",
        "        print(f\"Размер тестовой выборки: {X_test.shape}\")\n",
        "        print(f\"Класс 1 в тренировочной выборке: {y_train.mean():.3f} ({y_train.sum()}/{len(y_train)})\")\n",
        "        print(f\"Класс 1 в тестовой выборке: {y_test.mean():.3f} ({y_test.sum()}/{len(y_test)})\")\n",
        "else:\n",
        "    print(\"\\nПропускаем подготовку признаков: целевая переменная не найдена или невалидна\")\n",
        "    X_train, X_test, y_train, y_test = None, None, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8.2 Обучение и сравнение нескольких моделей\n",
        "if X_train is not None and y_train is not None:\n",
        "    print(\"\\n8.2 Обучение и сравнение моделей...\")\n",
        "    \n",
        "    # Определяем модели\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
        "        'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced_subsample', n_estimators=100),\n",
        "        'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
        "        'SVM': SVC(probability=True, random_state=42, class_weight='balanced', max_iter=1000)\n",
        "    }\n",
        "    \n",
        "    # Результаты\n",
        "    results = {}\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        print(f\"\\n--- {name} ---\")\n",
        "        \n",
        "        try:\n",
        "            # Кросс-валидация\n",
        "            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "            print(f\"ROC-AUC (кросс-валидация): {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})\")\n",
        "            \n",
        "            # Обучение на всей тренировочной выборке\n",
        "            model.fit(X_train, y_train)\n",
        "            \n",
        "            # Прогнозы\n",
        "            y_pred = model.predict(X_test)\n",
        "            y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
        "            \n",
        "            # Метрики\n",
        "            accuracy = np.mean(y_pred == y_test)\n",
        "            precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "            recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "            \n",
        "            roc_auc = None\n",
        "            if y_pred_proba is not None:\n",
        "                roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "                print(f\"ROC-AUC (тест): {roc_auc:.3f}\")\n",
        "            \n",
        "            print(f\"Accuracy: {accuracy:.3f}\")\n",
        "            print(f\"Precision: {precision:.3f}\")\n",
        "            print(f\"Recall: {recall:.3f}\")\n",
        "            print(f\"F1-Score: {f1:.3f}\")\n",
        "            \n",
        "            # Сохранение результатов\n",
        "            results[name] = {\n",
        "                'model': model,\n",
        "                'y_pred': y_pred,\n",
        "                'y_pred_proba': y_pred_proba,\n",
        "                'cv_score': cv_scores.mean(),\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'roc_auc': roc_auc\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при обучении {name}: {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    if len(results) == 0:\n",
        "        print(\"\\nВНИМАНИЕ: Не удалось обучить ни одной модели!\")\n",
        "    else:\n",
        "        print(f\"\\nУспешно обучено {len(results)} моделей\")\n",
        "else:\n",
        "    print(\"\\n8.2 Пропускаем обучение моделей: данные не подготовлены\")\n",
        "    results = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8.3 Визуализация результатов моделей\n",
        "if len(results) > 0 and X_test is not None and y_test is not None:\n",
        "    print(\"\\n8.3 Визуализация результатов моделей...\")\n",
        "    \n",
        "    # Создаем фигуру для визуализации\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    \n",
        "    # 8.3.1 Сравнение метрик моделей\n",
        "    metrics_comparison = pd.DataFrame({\n",
        "        'Model': list(results.keys()),\n",
        "        'ROC-AUC': [results[m]['roc_auc'] if results[m]['roc_auc'] is not None else 0 for m in results],\n",
        "        'Accuracy': [results[m]['accuracy'] for m in results],\n",
        "        'Precision': [results[m]['precision'] for m in results],\n",
        "        'Recall': [results[m]['recall'] for m in results],\n",
        "        'F1-Score': [results[m]['f1'] for m in results]\n",
        "    }).set_index('Model')\n",
        "    \n",
        "    # Heatmap сравнения метрик\n",
        "    sns.heatmap(metrics_comparison, annot=True, fmt='.3f', cmap='YlOrRd', ax=axes[0, 0])\n",
        "    axes[0, 0].set_title('Сравнение метрик моделей', fontweight='bold')\n",
        "    \n",
        "    # 8.3.2 ROC-кривые\n",
        "    axes[0, 1].plot([0, 1], [0, 1], 'k--', label='Случайная модель')\n",
        "    for name, result in results.items():\n",
        "        if result['y_pred_proba'] is not None:\n",
        "            fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            axes[0, 1].plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})', linewidth=2)\n",
        "    \n",
        "    axes[0, 1].set_xlim([0.0, 1.0])\n",
        "    axes[0, 1].set_ylim([0.0, 1.05])\n",
        "    axes[0, 1].set_xlabel('False Positive Rate', fontweight='bold')\n",
        "    axes[0, 1].set_ylabel('True Positive Rate', fontweight='bold')\n",
        "    axes[0, 1].set_title('ROC-кривые моделей', fontweight='bold')\n",
        "    axes[0, 1].legend(loc=\"lower right\")\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 8.3.3 Precision-Recall кривые\n",
        "    for name, result in results.items():\n",
        "        if result['y_pred_proba'] is not None:\n",
        "            precision_vals, recall_vals, _ = precision_recall_curve(y_test, result['y_pred_proba'])\n",
        "            axes[0, 2].plot(recall_vals, precision_vals, label=name, linewidth=2)\n",
        "    \n",
        "    axes[0, 2].set_xlabel('Recall', fontweight='bold')\n",
        "    axes[0, 2].set_ylabel('Precision', fontweight='bold')\n",
        "    axes[0, 2].set_title('Precision-Recall кривые', fontweight='bold')\n",
        "    axes[0, 2].legend(loc=\"upper right\")\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 8.3.4 Матрицы ошибок для лучшей модели по F1-Score\n",
        "    best_model_name = max(results.items(), key=lambda x: x[1]['f1'])[0]\n",
        "    best_result = results[best_model_name]\n",
        "    \n",
        "    cm = confusion_matrix(y_test, best_result['y_pred'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])\n",
        "    axes[1, 0].set_xlabel('Предсказанный класс', fontweight='bold')\n",
        "    axes[1, 0].set_ylabel('Истинный класс', fontweight='bold')\n",
        "    axes[1, 0].set_title(f'Матрица ошибок: {best_model_name}\\nF1-Score: {best_result[\"f1\"]:.3f}', fontweight='bold')\n",
        "    \n",
        "    # 8.3.5 Важность признаков для tree-based моделей\n",
        "    if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': X.columns,\n",
        "            'importance': best_result['model'].feature_importances_\n",
        "        }).sort_values('importance', ascending=False).head(15)\n",
        "        \n",
        "        axes[1, 1].barh(range(len(feature_importance)), \n",
        "                        feature_importance['importance'].values,\n",
        "                        color='skyblue')\n",
        "        axes[1, 1].set_yticks(range(len(feature_importance)))\n",
        "        axes[1, 1].set_yticklabels(feature_importance['feature'])\n",
        "        axes[1, 1].set_xlabel('Важность', fontweight='bold')\n",
        "        axes[1, 1].set_title(f'Топ-15 важных признаков: {best_model_name}', fontweight='bold')\n",
        "        axes[1, 1].invert_yaxis()\n",
        "    else:\n",
        "        # Для логистической регрессии - коэффициенты\n",
        "        if best_model_name == 'Logistic Regression':\n",
        "            coef_df = pd.DataFrame({\n",
        "                'feature': X.columns,\n",
        "                'coefficient': best_result['model'].coef_[0]\n",
        "            }).sort_values('coefficient', key=abs, ascending=False).head(15)\n",
        "            \n",
        "            colors = ['red' if x < 0 else 'green' for x in coef_df['coefficient']]\n",
        "            axes[1, 1].barh(range(len(coef_df)), coef_df['coefficient'].values, color=colors)\n",
        "            axes[1, 1].set_yticks(range(len(coef_df)))\n",
        "            axes[1, 1].set_yticklabels(coef_df['feature'])\n",
        "            axes[1, 1].set_xlabel('Коэффициент', fontweight='bold')\n",
        "            axes[1, 1].set_title(f'Топ-15 коэффициентов: {best_model_name}', fontweight='bold')\n",
        "            axes[1, 1].invert_yaxis()\n",
        "            axes[1, 1].axvline(x=0, color='black', linewidth=0.5)\n",
        "        else:\n",
        "            axes[1, 1].text(0.5, 0.5, 'Feature importance\\nnot available\\nfor this model',\n",
        "                           ha='center', va='center', transform=axes[1, 1].transAxes,\n",
        "                           fontsize=12)\n",
        "            axes[1, 1].set_title('Важность признаков', fontweight='bold')\n",
        "            axes[1, 1].axis('off')\n",
        "    \n",
        "    # 8.3.6 Сводная таблица лучшей модели\n",
        "    roc_auc_val = best_result['roc_auc'] if best_result['roc_auc'] is not None else 0\n",
        "    summary_text = f\"\"\"\n",
        "Лучшая модель: {best_model_name}\n",
        "\n",
        "Метрики:\n",
        "- ROC-AUC: {roc_auc_val:.3f}\n",
        "- Accuracy: {best_result['accuracy']:.3f}\n",
        "- Precision: {best_result['precision']:.3f}\n",
        "- Recall: {best_result['recall']:.3f}\n",
        "- F1-Score: {best_result['f1']:.3f}\n",
        "\n",
        "Кросс-валидация (ROC-AUC):\n",
        "Среднее: {best_result['cv_score']:.3f}\n",
        "\"\"\"\n",
        "    axes[1, 2].text(0.1, 0.5, summary_text, transform=axes[1, 2].transAxes,\n",
        "                    fontsize=10, verticalalignment='center',\n",
        "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    axes[1, 2].axis('off')\n",
        "    axes[1, 2].set_title('Сводка по лучшей модели', fontweight='bold')\n",
        "    \n",
        "    model_title = f'Результаты моделирования {target_var}' if target_var else 'Результаты моделирования'\n",
        "    plt.suptitle(model_title, fontsize=18, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_results_summary.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Визуализация завершена!\")\n",
        "else:\n",
        "    print(\"\\n8.3 Пропускаем визуализацию: нет обученных моделей или данных\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. СВОДНЫЙ АНАЛИЗ И ВЫВОДЫ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9. СВОДНЫЙ АНАЛИЗ И ВЫВОДЫ\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"9. СВОДНЫЙ АНАЛИЗ И ВЫВОДЫ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Пересчитываем ключевые статистики ПОСЛЕ всех изменений данных\n",
        "print(\"\\n9.1 КЛЮЧЕВЫЕ ВЫВОДЫ И РЕКОМЕНДАЦИИ\\n\")\n",
        "\n",
        "# Пересчитываем статистики целевой переменной\n",
        "print(\"1. БАЗОВАЯ СТАТИСТИКА:\")\n",
        "if target_var and target_var in df.columns:\n",
        "    # Текущая статистика после всех обработок\n",
        "    current_target_counts = df[target_var].value_counts()\n",
        "    current_target_percentage = df[target_var].value_counts(normalize=True) * 100\n",
        "    \n",
        "    # Проверяем формат целевой переменной (числовой или строковый)\n",
        "    target_values = df[target_var].unique()\n",
        "    \n",
        "    # Определяем метки для классов\n",
        "    if df[target_var].dtype == 'object':\n",
        "        # Строковый формат - ищем положительный класс\n",
        "        positive_labels = ['Yes', 'YES', 'yes', '1', 'True', 'true', 'Positive', 'positive']\n",
        "        positive_label = None\n",
        "        for label in positive_labels:\n",
        "            if label in df[target_var].values:\n",
        "                positive_label = label\n",
        "                break\n",
        "        if positive_label is None:\n",
        "            # Берем первое значение как положительный класс\n",
        "            positive_label = current_target_counts.index[0]\n",
        "    else:\n",
        "        # Числовой формат\n",
        "        positive_label = 1\n",
        "    \n",
        "    # Получаем значения для положительного класса\n",
        "    if positive_label in current_target_counts.index:\n",
        "        target_positive = current_target_counts[positive_label]\n",
        "        target_positive_pct = current_target_percentage[positive_label]\n",
        "    else:\n",
        "        # Если нет, ищем альтернативные обозначения\n",
        "        target_positive = 0\n",
        "        target_positive_pct = 0\n",
        "        for val in current_target_counts.index:\n",
        "            if str(val).lower() in ['yes', 'y', '1', 1, 'true', 'positive']:\n",
        "                target_positive = current_target_counts[val]\n",
        "                target_positive_pct = current_target_percentage[val]\n",
        "                break\n",
        "    \n",
        "    print(f\"   • Распределение целевой переменной {target_var}:\")\n",
        "    for val, count in current_target_counts.items():\n",
        "        pct = current_target_percentage[val]\n",
        "        print(f\"     - {val}: {count} ({pct:.1f}%)\")\n",
        "    print(f\"   • Положительный класс: {target_positive_pct:.1f}% ({target_positive} наблюдений)\")\n",
        "else:\n",
        "    print(\"   • Целевая переменная: не найдена\")\n",
        "    target_positive = 0\n",
        "    target_positive_pct = 0\n",
        "\n",
        "print(f\"   • Размер выборки: {len(df)} наблюдений\")\n",
        "print(f\"   • Количество признаков: {df.shape[1]}\")\n",
        "\n",
        "print(f\"\\n2. КЛЮЧЕВЫЕ ФАКТОРЫ, ВЛИЯЮЩИЕ НА {target_var if target_var else 'ЦЕЛЕВУЮ ПЕРЕМЕННУЮ'}:\")\n",
        "\n",
        "# Факторы из корреляционного анализа\n",
        "if 'target_corr' in locals() and target_corr is not None and len(target_corr) > 0:\n",
        "    print(\"   А) Числовые переменные (наибольшая корреляция):\")\n",
        "    \n",
        "    # Исключаем саму целевую переменную если она есть\n",
        "    if target_var_numeric and target_var_numeric in target_corr.index:\n",
        "        relevant_corr = target_corr.drop(target_var_numeric)\n",
        "    else:\n",
        "        relevant_corr = target_corr\n",
        "    \n",
        "    # Сортируем по абсолютному значению\n",
        "    top_corr_abs = relevant_corr.abs().sort_values(ascending=False).head(5)\n",
        "    \n",
        "    for i, (var, corr_abs) in enumerate(top_corr_abs.items(), 1):\n",
        "        # Получаем реальное значение корреляции\n",
        "        corr_value = relevant_corr[var]\n",
        "        direction = \"увеличивает\" if corr_value > 0 else \"уменьшает\"\n",
        "        print(f\"      {i}. {var}: {corr_value:.3f} ({direction} вероятность положительного класса)\")\n",
        "else:\n",
        "    print(\"   А) Числовые переменные: данные корреляционного анализа недоступны\")\n",
        "\n",
        "# Бинарные переменные\n",
        "if 'odds_df' in locals() and not odds_df.empty and len(odds_df) > 0:\n",
        "    print(\"\\n   Б) Бинарные переменные (наибольшее влияние):\")\n",
        "    top_binary = odds_df.head(3)\n",
        "    \n",
        "    for i, row in top_binary.iterrows():\n",
        "        if 'odds_ratio' in row:\n",
        "            if row['odds_ratio'] > 1:\n",
        "                effect = \"повышает\"\n",
        "                effect_text = f\"в {row['odds_ratio']:.1f} раз\"\n",
        "            else:\n",
        "                effect = \"снижает\"\n",
        "                effect_text = f\"в {1/row['odds_ratio']:.1f} раз\"\n",
        "            \n",
        "            # Укорачиваем название переменной если оно слишком длинное\n",
        "            var_name = row['variable']\n",
        "            if len(var_name) > 30:\n",
        "                var_name = var_name[:27] + \"...\"\n",
        "            \n",
        "            print(f\"      {i+1}. {var_name}: OR={row['odds_ratio']:.2f} ({effect} вероятность {effect_text})\")\n",
        "else:\n",
        "    print(\"\\n   Б) Бинарные переменные: данные анализа odds ratios недоступны\")\n",
        "\n",
        "# Кластерный анализ\n",
        "print(\"\\n3. КЛАСТЕРНЫЙ АНАЛИЗ:\")\n",
        "if 'Cluster' in df.columns and 'optimal_k' in locals():\n",
        "    print(f\"   • Оптимальное число кластеров: {optimal_k}\")\n",
        "    print(\"   • Распределение целевой переменной по кластерам:\")\n",
        "    \n",
        "    cluster_analysis = []\n",
        "    for cluster in sorted(df['Cluster'].dropna().unique()):\n",
        "        cluster_size = (df['Cluster'] == cluster).sum()\n",
        "        \n",
        "        # Рассчитываем долю положительного класса для кластера\n",
        "        cluster_data = df[df['Cluster'] == cluster]\n",
        "        if target_var and target_var in cluster_data.columns:\n",
        "            # Определяем формат целевой переменной\n",
        "            if cluster_data[target_var].dtype == 'object':\n",
        "                # Ищем положительный класс\n",
        "                positive_count = 0\n",
        "                for val in cluster_data[target_var].dropna():\n",
        "                    if str(val).lower() in ['yes', 'y', '1', 'true', 'positive']:\n",
        "                        positive_count += 1\n",
        "            else:\n",
        "                # Числовой формат\n",
        "                positive_count = (cluster_data[target_var] == 1).sum()\n",
        "            \n",
        "            positive_rate = (positive_count / cluster_size * 100) if cluster_size > 0 else 0\n",
        "            cluster_analysis.append((cluster, cluster_size, positive_rate))\n",
        "    \n",
        "    # Сортируем по доле положительного класса (от высокой к низкой)\n",
        "    cluster_analysis.sort(key=lambda x: x[2], reverse=True)\n",
        "    \n",
        "    for cluster, size, rate in cluster_analysis:\n",
        "        print(f\"      Кластер {cluster}: {size} наблюдений ({rate:.1f}% положительного класса)\")\n",
        "else:\n",
        "    print(\"   • Кластерный анализ: не выполнен\")\n",
        "\n",
        "print(\"\\n4. РЕЗУЛЬТАТЫ МОДЕЛИРОВАНИЯ:\")\n",
        "if 'best_model_name' in locals() and 'best_result' in locals():\n",
        "    print(f\"   • Лучшая модель: {best_model_name}\")\n",
        "    print(f\"   • F1-Score: {best_result['f1']:.3f}\")\n",
        "    \n",
        "    if best_result['roc_auc'] is not None:\n",
        "        print(f\"   • ROC-AUC: {best_result['roc_auc']:.3f}\")\n",
        "    else:\n",
        "        print(f\"   • ROC-AUC: не рассчитано\")\n",
        "    \n",
        "    print(f\"   • Accuracy: {best_result['accuracy']:.3f}\")\n",
        "    print(f\"   • Precision: {best_result['precision']:.3f}\")\n",
        "    print(f\"   • Recall: {best_result['recall']:.3f}\")\n",
        "else:\n",
        "    print(\"   • Результаты моделирования: недоступны\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9.2 Практические рекомендации\n",
        "print(\"\\n9.2 ПРАКТИЧЕСКИЕ РЕКОМЕНДАЦИИ:\\n\")\n",
        "\n",
        "print(\"1. ПРИОРИТЕТНЫЕ ГРУППЫ РИСКА:\")\n",
        "if target_var:\n",
        "    print(f\"   • Наблюдения с факторами, сильно коррелирующими с {target_var}\")\n",
        "    if 'top_corr_abs' in locals() and len(top_corr_abs) > 0:\n",
        "        print(f\"   • Особое внимание к переменным: {', '.join(top_corr_abs.head(3).index.tolist())}\")\n",
        "if 'Cluster' in df.columns:\n",
        "    print(f\"   • Кластер с наибольшей долей положительного класса (если анализ выполнен)\")\n",
        "\n",
        "print(\"\\n2. МЕРЫ ПО УЛУЧШЕНИЮ:\")\n",
        "if target_var:\n",
        "    print(f\"   • Анализ и коррекция факторов из топ-корреляций с {target_var}\")\n",
        "    print(\"   • Регулярный мониторинг ключевых метрик\")\n",
        "    if 'odds_df' in locals() and not odds_df.empty:\n",
        "        print(\"   • Фокус на бинарных переменных с высоким odds ratio\")\n",
        "\n",
        "print(\"\\n3. МОНИТОРИНГ:\")\n",
        "print(\"   • Регулярный анализ ключевых метрик из данного отчета\")\n",
        "if 'Cluster' in df.columns:\n",
        "    print(\"   • Фокус на наблюдениях из 'рисковых' кластеров\")\n",
        "print(\"   • A/B тестирование мер по улучшению\")\n",
        "if target_positive_pct > 0:\n",
        "    print(f\"   • Мониторинг доли положительного класса (текущее значение: {target_positive_pct:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. СОХРАНЕНИЕ РЕЗУЛЬТАТОВ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10. СОХРАНЕНИЕ РЕЗУЛЬТАТОВ\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"10. СОХРАНЕНИЕ РЕЗУЛЬТАТОВ АНАЛИЗА\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Функция для расчета статистики целевой переменной\n",
        "def get_target_stats(df, target_var):\n",
        "    if not target_var or target_var not in df.columns:\n",
        "        return 0, 0, 0\n",
        "    \n",
        "    # Приводим к строковому формату\n",
        "    target_series = df[target_var].astype(str).str.strip().str.title()\n",
        "    \n",
        "    # Определяем положительный класс\n",
        "    positive_labels = ['Yes', 'YES', 'yes', '1', 'True', 'true', 'Positive', 'positive']\n",
        "    positive_count = 0\n",
        "    negative_count = 0\n",
        "    \n",
        "    for val in target_series.dropna():\n",
        "        if str(val).lower() in [l.lower() for l in positive_labels]:\n",
        "            positive_count += 1\n",
        "        else:\n",
        "            negative_count += 1\n",
        "    \n",
        "    total = positive_count + negative_count\n",
        "    \n",
        "    if total > 0:\n",
        "        positive_pct = (positive_count / total) * 100\n",
        "    else:\n",
        "        positive_pct = 0\n",
        "    \n",
        "    return positive_count, positive_pct, total\n",
        "\n",
        "# Получаем актуальные статистики\n",
        "if target_var and target_var in df.columns:\n",
        "    current_positive_count, current_positive_pct, current_total = get_target_stats(df, target_var)\n",
        "else:\n",
        "    current_positive_count, current_positive_pct, current_total = 0, 0, 0\n",
        "\n",
        "# Определяем базовое имя для файлов\n",
        "base_name = target_var.lower().replace(' ', '_') if target_var else 'analysis'\n",
        "\n",
        "# Сохранение обработанных данных\n",
        "processed_filename = f'{base_name}_processed.csv'\n",
        "df.to_csv(processed_filename, index=False)\n",
        "print(f\"✓ Обработанные данные сохранены в: {processed_filename}\")\n",
        "\n",
        "# Сохранение ключевых метрик\n",
        "summary_data = {\n",
        "    'Метрика': [\n",
        "        f'Положительный класс {target_var if target_var else \"целевой переменной\"} (%)',\n",
        "        'Количество положительного класса',\n",
        "        'Общее количество наблюдений',\n",
        "        'Размер выборки (строки)',\n",
        "        'Количество признаков (столбцы)',\n",
        "        'Оптимальное число кластеров',\n",
        "        'Лучшая модель',\n",
        "        'F1-Score лучшей модели',\n",
        "        'ROC-AUC лучшей модели',\n",
        "        'Accuracy лучшей модели',\n",
        "        'Precision лучшей модели',\n",
        "        'Recall лучшей модели'\n",
        "    ],\n",
        "    'Значение': [\n",
        "        current_positive_pct,\n",
        "        int(current_positive_count),\n",
        "        int(current_total),\n",
        "        len(df),\n",
        "        df.shape[1],\n",
        "        optimal_k if 'optimal_k' in locals() else 'N/A',\n",
        "        best_model_name if 'best_model_name' in locals() else 'N/A',\n",
        "        best_result['f1'] if 'best_result' in locals() else 'N/A',\n",
        "        best_result['roc_auc'] if 'best_result' in locals() and best_result['roc_auc'] is not None else 'N/A',\n",
        "        best_result['accuracy'] if 'best_result' in locals() else 'N/A',\n",
        "        best_result['precision'] if 'best_result' in locals() else 'N/A',\n",
        "        best_result['recall'] if 'best_result' in locals() else 'N/A'\n",
        "    ]\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_filename = f'{base_name}_summary.csv'\n",
        "summary_df.to_csv(summary_filename, index=False)\n",
        "print(f\"✓ Сводные метрики сохранены в: {summary_filename}\")\n",
        "\n",
        "# Сохранение корреляций (если они есть)\n",
        "if 'target_corr' in locals() and target_corr is not None and len(target_corr) > 0:\n",
        "    try:\n",
        "        target_corr_df = pd.DataFrame({\n",
        "            'Признак': target_corr.index,\n",
        "            f'Корреляция_с_{target_var if target_var else \"целевой_переменной\"}': target_corr.values\n",
        "        })\n",
        "        corr_filename = f'{base_name}_correlations.csv'\n",
        "        target_corr_df.to_csv(corr_filename, index=False)\n",
        "        print(f\"✓ Корреляции сохранены в: {corr_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Ошибка при сохранении корреляций: {e}\")\n",
        "else:\n",
        "    print(\"✗ Корреляции не найдены для сохранения\")\n",
        "\n",
        "# Сохранение отношений шансов (если они есть)\n",
        "if 'odds_df' in locals() and not odds_df.empty and len(odds_df) > 0:\n",
        "    try:\n",
        "        odds_filename = f'{base_name}_odds_ratios.csv'\n",
        "        odds_df.to_csv(odds_filename, index=False)\n",
        "        print(f\"✓ Отношения шансов сохранены в: {odds_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Ошибка при сохранении отношений шансов: {e}\")\n",
        "else:\n",
        "    print(\"✗ Отношения шансов не найдены для сохранения\")\n",
        "\n",
        "# Сохранение профилей кластеров (если кластеризация выполнена)\n",
        "if 'Cluster' in df.columns:\n",
        "    try:\n",
        "        # Создаем профили кластеров, если их еще нет\n",
        "        if 'available_features' in locals() and len(available_features) > 0:\n",
        "            df_clustered = df[df['Cluster'].notna()].copy()\n",
        "            if len(df_clustered) > 0:\n",
        "                cluster_profiles = df_clustered.groupby('Cluster')[available_features].mean()\n",
        "                cluster_filename = f'{base_name}_cluster_profiles.csv'\n",
        "                cluster_profiles.to_csv(cluster_filename)\n",
        "                print(f\"✓ Профили кластеров сохранены в: {cluster_filename}\")\n",
        "            else:\n",
        "                print(\"✗ Профили кластеров не найдены для сохранения (нет данных с кластерами)\")\n",
        "        else:\n",
        "            print(\"✗ Профили кластеров не найдены для сохранения\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Ошибка при сохранении профилей кластеров: {e}\")\n",
        "else:\n",
        "    print(\"✗ Профили кластеров не найдены для сохранения\")\n",
        "\n",
        "# Дополнительно: сохранение важных признаков из лучшей модели\n",
        "if 'best_model_name' in locals() and 'best_result' in locals() and 'X' in locals():\n",
        "    try:\n",
        "        if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
        "            feature_importance = pd.DataFrame({\n",
        "                'feature': X.columns,\n",
        "                'importance': best_result['model'].feature_importances_\n",
        "            }).sort_values('importance', ascending=False)\n",
        "            feature_filename = f'{base_name}_feature_importance.csv'\n",
        "            feature_importance.to_csv(feature_filename, index=False)\n",
        "            print(f\"✓ Важность признаков сохранена в: {feature_filename}\")\n",
        "        elif best_model_name == 'Logistic Regression':\n",
        "            coef_df = pd.DataFrame({\n",
        "                'feature': X.columns,\n",
        "                'coefficient': best_result['model'].coef_[0],\n",
        "                'abs_coefficient': np.abs(best_result['model'].coef_[0])\n",
        "            }).sort_values('abs_coefficient', ascending=False)\n",
        "            coef_filename = f'{base_name}_logistic_coefficients.csv'\n",
        "            coef_df.to_csv(coef_filename, index=False)\n",
        "            print(f\"✓ Коэффициенты логистической регрессии сохранены в: {coef_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Ошибка при сохранении важности признаков: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"АНАЛИЗ УСПЕШНО ЗАВЕРШЕН!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nСОЗДАННЫЕ ФАЙЛЫ:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "import os\n",
        "created_files = []\n",
        "\n",
        "# Проверяем созданные файлы\n",
        "file_checks = [\n",
        "    # CSV файлы\n",
        "    (f'{base_name}_processed.csv', 'Обработанные данные'),\n",
        "    (f'{base_name}_summary.csv', 'Сводные метрики'),\n",
        "    (f'{base_name}_correlations.csv', f'Корреляции с {target_var if target_var else \"целевой переменной\"}'),\n",
        "    (f'{base_name}_odds_ratios.csv', 'Отношения шансов'),\n",
        "    (f'{base_name}_cluster_profiles.csv', 'Профили кластеров'),\n",
        "    (f'{base_name}_feature_importance.csv', 'Важность признаков'),\n",
        "    (f'{base_name}_logistic_coefficients.csv', 'Коэффициенты регрессии'),\n",
        "    # Графики\n",
        "    ('class_balance.png', 'График: Баланс классов'),\n",
        "    ('continuous_vars_distribution.png', 'График: Распределение числовых переменных'),\n",
        "    ('ordinal_vars_distribution.png', 'График: Распределение порядковых переменных'),\n",
        "    ('target_comparison_ttest.png', 'График: Сравнение по целевой переменной'),\n",
        "    ('full_correlation_heatmap.png', 'График: Корреляционная матрица'),\n",
        "    ('top_correlations_with_target.png', 'График: Топ корреляций'),\n",
        "    ('odds_ratios_binary_vars.png', 'График: Odds Ratios'),\n",
        "    ('categorical_vars_target.png', 'График: Категориальные переменные'),\n",
        "    ('optimal_clusters_determination.png', 'График: Определение кластеров'),\n",
        "    ('clustering_results.png', 'График: Результаты кластеризации'),\n",
        "    ('cluster_profiles_heatmap.png', 'График: Профили кластеров'),\n",
        "    ('pca_variance_analysis.png', 'График: PCA анализ'),\n",
        "    ('pca_loadings_analysis.png', 'График: Нагрузки PCA'),\n",
        "    ('pca_biplot.png', 'График: Biplot PCA'),\n",
        "    ('model_results_summary.png', 'График: Результаты моделей'),\n",
        "    ('final_analysis_dashboard.png', 'Дашборд: Итоговый дашборд'),\n",
        "    ('missing_values_heatmap.png', 'График: Пропущенные значения'),\n",
        "]\n",
        "\n",
        "for filename, description in file_checks:\n",
        "    if os.path.exists(filename):\n",
        "        file_size = os.path.getsize(filename) / 1024  # Размер в КБ\n",
        "        created_files.append((filename, description, file_size))\n",
        "        print(f\"✓ {description:45} [{filename}] ({file_size:.1f} KB)\")\n",
        "    else:\n",
        "        print(f\"✗ {description:45} [{filename}] (не создан)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 40)\n",
        "print(f\"ИТОГО: {len(created_files)} файлов создано успешно\")\n",
        "\n",
        "if created_files:\n",
        "    total_size = sum(f[2] for f in created_files)\n",
        "    print(f\"Общий размер: {total_size:.1f} KB\")\n",
        "\n",
        "print(\"\\nКЛЮЧЕВЫЕ РЕЗУЛЬТАТЫ:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Выводим ключевые результаты\n",
        "if current_total > 0 and target_var:\n",
        "    print(f\"• Распределение {target_var}: {current_positive_pct:.1f}% положительного класса ({current_positive_count}/{current_total})\")\n",
        "elif target_var:\n",
        "    print(f\"• Целевая переменная {target_var}: данные недоступны\")\n",
        "else:\n",
        "    print(\"• Целевая переменная: не найдена\")\n",
        "\n",
        "if 'optimal_k' in locals():\n",
        "    print(f\"• Оптимальное число кластеров: {optimal_k}\")\n",
        "\n",
        "if 'best_model_name' in locals() and 'best_result' in locals():\n",
        "    print(f\"• Лучшая модель предсказания: {best_model_name}\")\n",
        "    print(f\"• Качество модели (F1-Score): {best_result['f1']:.3f}\")\n",
        "\n",
        "print(\"\\nРЕКОМЕНДАЦИИ:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"1. Проанализируйте файл {summary_filename} для основных метрик\")\n",
        "if 'target_corr' in locals() and target_corr is not None:\n",
        "    print(f\"2. Изучите {base_name}_correlations.csv для понимания факторов влияния\")\n",
        "if 'Cluster' in df.columns:\n",
        "    print(f\"3. Используйте {base_name}_cluster_profiles.csv для сегментации наблюдений\")\n",
        "print(\"4. Все графики сохранены в PNG файлах для презентаций и отчетов\")\n",
        "print(f\"5. Обработанные данные в {processed_filename} можно использовать для дальнейшего анализа\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Анализ завершен! Результаты готовы для использования.\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9.3 Создание финального дашборда\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"СОЗДАНИЕ ФИНАЛЬНОГО ДАШБОРДА...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Проверяем наличие необходимых данных\n",
        "has_target = target_var and target_var in df.columns\n",
        "has_corr = 'target_corr' in locals() and target_corr is not None and len(target_corr) > 0\n",
        "has_cluster = 'Cluster' in df.columns\n",
        "has_results = 'results' in locals() and len(results) > 0\n",
        "has_best_model = 'best_model_name' in locals() and 'best_result' in locals()\n",
        "\n",
        "if has_target or has_corr or has_cluster or has_results:\n",
        "    # Создаем финальный дашборд\n",
        "    fig = plt.figure(figsize=(20, 15))\n",
        "    \n",
        "    plot_idx = 0\n",
        "    \n",
        "    # 1. Распределение целевой переменной (левый верхний)\n",
        "    if has_target:\n",
        "        ax1 = plt.subplot2grid((4, 4), (0, 0), colspan=2)\n",
        "        target_counts_plot = df[target_var].value_counts()\n",
        "        if len(target_counts_plot) == 2:\n",
        "            target_counts_plot.plot(kind='pie', ax=ax1, autopct='%1.1f%%', \n",
        "                                  colors=['#4CAF50', '#F44336'], startangle=90,\n",
        "                                  explode=(0, 0.1), textprops={'fontsize': 12})\n",
        "        else:\n",
        "            target_counts_plot.plot(kind='pie', ax=ax1, autopct='%1.1f%%', \n",
        "                                  startangle=90, textprops={'fontsize': 12})\n",
        "        ax1.set_ylabel('')\n",
        "        ax1.set_title(f'Распределение {target_var}', fontsize=14, fontweight='bold')\n",
        "        plot_idx += 1\n",
        "    \n",
        "    # 2. Топ корреляций (правый верхний)\n",
        "    if has_corr:\n",
        "        ax2 = plt.subplot2grid((4, 4), (0, 2), colspan=2)\n",
        "        if target_var_numeric and target_var_numeric in target_corr.index:\n",
        "            top_corr_plot = target_corr.drop(target_var_numeric).head(8)\n",
        "        else:\n",
        "            top_corr_plot = target_corr.head(8)\n",
        "        \n",
        "        if len(top_corr_plot) > 0:\n",
        "            colors_corr = ['red' if x < 0 else 'green' for x in top_corr_plot.values]\n",
        "            bars = ax2.barh(range(len(top_corr_plot)), top_corr_plot.values, color=colors_corr, alpha=0.7)\n",
        "            ax2.set_yticks(range(len(top_corr_plot)))\n",
        "            ax2.set_yticklabels(top_corr_plot.index, fontsize=10)\n",
        "            ax2.set_xlabel(f'Корреляция с {target_var if target_var else \"целевой переменной\"}', fontweight='bold')\n",
        "            ax2.set_title('Топ факторов влияния', fontsize=14, fontweight='bold')\n",
        "            ax2.axvline(x=0, color='black', linewidth=0.5)\n",
        "    \n",
        "    # 3. Кластерный анализ (левый средний)\n",
        "    if has_cluster:\n",
        "        ax3 = plt.subplot2grid((4, 4), (1, 0), colspan=2)\n",
        "        cluster_dist = df['Cluster'].dropna().value_counts().sort_index()\n",
        "        cluster_target_rates = []\n",
        "        \n",
        "        for cluster in cluster_dist.index:\n",
        "            cluster_data = df[df['Cluster'] == cluster]\n",
        "            if has_target and target_var in cluster_data.columns:\n",
        "                if cluster_data[target_var].dtype == 'object':\n",
        "                    positive_count = sum(1 for val in cluster_data[target_var].dropna() \n",
        "                                       if str(val).lower() in ['yes', 'y', '1', 'true', 'positive'])\n",
        "                else:\n",
        "                    positive_count = (cluster_data[target_var] == 1).sum()\n",
        "                rate = (positive_count / len(cluster_data) * 100) if len(cluster_data) > 0 else 0\n",
        "            else:\n",
        "                rate = 0\n",
        "            cluster_target_rates.append(rate)\n",
        "        \n",
        "        x = range(len(cluster_dist))\n",
        "        bars_cluster = ax3.bar(x, cluster_dist.values, alpha=0.7, color='skyblue')\n",
        "        ax3.set_xlabel('Кластер', fontweight='bold')\n",
        "        ax3.set_ylabel('Количество наблюдений', fontweight='bold', color='skyblue')\n",
        "        ax3.set_xticks(x)\n",
        "        ax3.set_xticklabels([f'Кластер {i}' for i in cluster_dist.index])\n",
        "        \n",
        "        # Вторая ось для процента положительного класса\n",
        "        if len(cluster_target_rates) > 0:\n",
        "            ax3_2 = ax3.twinx()\n",
        "            ax3_2.plot(x, cluster_target_rates, 'ro-', linewidth=2, markersize=8)\n",
        "            ax3_2.set_ylabel('Процент положительного класса (%)', fontweight='bold', color='red')\n",
        "            ax3_2.tick_params(axis='y', labelcolor='red')\n",
        "        ax3.set_title('Кластерный анализ', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # 4. Моделирование (правый средний)\n",
        "    if has_results:\n",
        "        ax4 = plt.subplot2grid((4, 4), (1, 2), colspan=2)\n",
        "        model_names = list(results.keys())\n",
        "        f1_scores = [results[m]['f1'] for m in model_names]\n",
        "        roc_auc_scores = [results[m]['roc_auc'] if results[m]['roc_auc'] is not None else 0 for m in model_names]\n",
        "        \n",
        "        x_model = np.arange(len(model_names))\n",
        "        width = 0.35\n",
        "        \n",
        "        bars_f1 = ax4.bar(x_model - width/2, f1_scores, width, label='F1-Score', alpha=0.7, color='orange')\n",
        "        bars_auc = ax4.bar(x_model + width/2, roc_auc_scores, width, label='ROC-AUC', alpha=0.7, color='green')\n",
        "        \n",
        "        ax4.set_xlabel('Модель', fontweight='bold')\n",
        "        ax4.set_ylabel('Score', fontweight='bold')\n",
        "        ax4.set_xticks(x_model)\n",
        "        ax4.set_xticklabels(model_names, rotation=45, ha='right')\n",
        "        ax4.legend()\n",
        "        ax4.set_title('Сравнение моделей', fontsize=14, fontweight='bold')\n",
        "        ax4.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # 5. Важные признаки (нижний левый)\n",
        "    ax5 = plt.subplot2grid((4, 4), (2, 0), colspan=2, rowspan=2)\n",
        "    \n",
        "    if has_best_model and 'X' in locals() and best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
        "        # Используем feature importance из лучшей модели\n",
        "        feature_importance_plot = pd.DataFrame({\n",
        "            'feature': X.columns,\n",
        "            'importance': best_result['model'].feature_importances_\n",
        "        }).sort_values('importance', ascending=False).head(10)\n",
        "        \n",
        "        ax5.barh(range(len(feature_importance_plot)), feature_importance_plot['importance'].values,\n",
        "                color=plt.cm.viridis(np.linspace(0, 1, len(feature_importance_plot))))\n",
        "        ax5.set_yticks(range(len(feature_importance_plot)))\n",
        "        ax5.set_yticklabels(feature_importance_plot['feature'], fontsize=10)\n",
        "    elif has_best_model and 'X' in locals() and best_model_name == 'Logistic Regression':\n",
        "        # Используем коэффициенты из логистической регрессии\n",
        "        coef_abs = pd.DataFrame({\n",
        "            'feature': X.columns,\n",
        "            'abs_coef': np.abs(best_result['model'].coef_[0])\n",
        "        }).sort_values('abs_coef', ascending=False).head(10)\n",
        "        \n",
        "        ax5.barh(range(len(coef_abs)), coef_abs['abs_coef'].values,\n",
        "                color=plt.cm.viridis(np.linspace(0, 1, len(coef_abs))))\n",
        "        ax5.set_yticks(range(len(coef_abs)))\n",
        "        ax5.set_yticklabels(coef_abs['feature'], fontsize=10)\n",
        "    elif has_corr:\n",
        "        # Альтернатива: используем корреляции\n",
        "        if target_var_numeric and target_var_numeric in target_corr.index:\n",
        "            top_corr_abs = target_corr.drop(target_var_numeric).abs().sort_values(ascending=False).head(10)\n",
        "        else:\n",
        "            top_corr_abs = target_corr.abs().sort_values(ascending=False).head(10)\n",
        "        \n",
        "        if len(top_corr_abs) > 0:\n",
        "            ax5.barh(range(len(top_corr_abs)), top_corr_abs.values,\n",
        "                    color=plt.cm.viridis(np.linspace(0, 1, len(top_corr_abs))))\n",
        "            ax5.set_yticks(range(len(top_corr_abs)))\n",
        "            ax5.set_yticklabels(top_corr_abs.index, fontsize=10)\n",
        "        else:\n",
        "            ax5.text(0.5, 0.5, 'Данные недоступны', ha='center', va='center', transform=ax5.transAxes)\n",
        "    else:\n",
        "        ax5.text(0.5, 0.5, 'Данные недоступны', ha='center', va='center', transform=ax5.transAxes)\n",
        "    \n",
        "    ax5.set_xlabel('Важность/Влияние', fontweight='bold')\n",
        "    ax5.set_title('Топ-10 важных признаков', fontsize=14, fontweight='bold')\n",
        "    ax5.invert_yaxis()\n",
        "    \n",
        "    # 6. Демографический анализ (нижний правый) - если есть соответствующие колонки\n",
        "    ax6 = plt.subplot2grid((4, 4), (2, 2), colspan=2, rowspan=2)\n",
        "    \n",
        "    # Проверяем наличие колонок для демографического анализа\n",
        "    age_col = None\n",
        "    tenure_col = None\n",
        "    \n",
        "    # Ищем колонки с возрастом\n",
        "    for col in ['Age', 'age', 'AGE', 'Возраст', 'возраст']:\n",
        "        if col in df.columns:\n",
        "            age_col = col\n",
        "            break\n",
        "    \n",
        "    # Ищем колонки со стажем\n",
        "    for col in ['YearsAtCompany', 'years_at_company', 'Tenure', 'tenure', 'Стаж', 'стаж']:\n",
        "        if col in df.columns:\n",
        "            tenure_col = col\n",
        "            break\n",
        "    \n",
        "    if has_target and (age_col or tenure_col):\n",
        "        plot_data = []\n",
        "        labels = []\n",
        "        \n",
        "        # Определяем положительный класс для анализа\n",
        "        if df[target_var].dtype == 'object':\n",
        "            positive_labels = ['Yes', 'YES', 'yes', '1', 'True', 'true', 'Positive', 'positive']\n",
        "            positive_label_plot = None\n",
        "            for label in positive_labels:\n",
        "                if label in df[target_var].values:\n",
        "                    positive_label_plot = label\n",
        "                    break\n",
        "            if positive_label_plot is None:\n",
        "                positive_label_plot = df[target_var].value_counts().index[0]\n",
        "        else:\n",
        "            positive_label_plot = 1\n",
        "        \n",
        "        if age_col:\n",
        "            try:\n",
        "                age_groups = pd.cut(df[age_col].dropna(), bins=5)\n",
        "                age_target = df.groupby(age_groups)[target_var].apply(\n",
        "                    lambda x: (x == positive_label_plot).mean() * 100 if has_target else 0\n",
        "                )\n",
        "                plot_data.append(age_target.values)\n",
        "                labels.append('По возрасту')\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        if tenure_col:\n",
        "            try:\n",
        "                tenure_groups = pd.cut(df[tenure_col].dropna(), bins=5)\n",
        "                tenure_target = df.groupby(tenure_groups)[target_var].apply(\n",
        "                    lambda x: (x == positive_label_plot).mean() * 100 if has_target else 0\n",
        "                )\n",
        "                plot_data.append(tenure_target.values)\n",
        "                labels.append('По стажу')\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        if len(plot_data) > 0:\n",
        "            x_pos = range(len(plot_data[0]))\n",
        "            width = 0.4\n",
        "            \n",
        "            for idx, (data, label) in enumerate(zip(plot_data, labels)):\n",
        "                x_offset = [i + idx * width for i in x_pos]\n",
        "                color = 'blue' if idx == 0 else 'red'\n",
        "                ax6.bar(x_offset, data, width=width, label=label, alpha=0.7, color=color)\n",
        "            \n",
        "            ax6.set_xlabel('Группы', fontweight='bold')\n",
        "            ax6.set_ylabel('Процент положительного класса (%)', fontweight='bold')\n",
        "            ax6.set_xticks([i + width/2 for i in x_pos])\n",
        "            if len(plot_data) > 0:\n",
        "                group_labels = [str(age_groups.categories[i]) if age_col else str(tenure_groups.categories[i]) \n",
        "                              for i in range(min(len(plot_data[0]), 5))]\n",
        "                ax6.set_xticklabels(group_labels, rotation=45, ha='right', fontsize=9)\n",
        "            ax6.legend()\n",
        "            ax6.set_title('Распределение по группам', fontsize=14, fontweight='bold')\n",
        "            ax6.grid(axis='y', alpha=0.3)\n",
        "        else:\n",
        "            ax6.text(0.5, 0.5, 'Демографический анализ\\nнедоступен', \n",
        "                    ha='center', va='center', transform=ax6.transAxes, fontsize=12)\n",
        "            ax6.set_title('Демографический анализ', fontsize=14, fontweight='bold')\n",
        "    else:\n",
        "        ax6.text(0.5, 0.5, 'Демографический анализ\\nнедоступен', \n",
        "                ha='center', va='center', transform=ax6.transAxes, fontsize=12)\n",
        "        ax6.set_title('Демографический анализ', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    dashboard_title = f'ДАШБОРД: АНАЛИЗ {target_var.upper()}' if target_var else 'ДАШБОРД: ФИНАЛЬНЫЙ АНАЛИЗ'\n",
        "    plt.suptitle(dashboard_title, fontsize=20, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('final_analysis_dashboard.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Финальный дашборд создан и сохранен!\")\n",
        "else:\n",
        "    print(\"Недостаточно данных для создания дашборда\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
